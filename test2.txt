WORDS=28
Filtering out chunk, density_ratio=0.42857142857142855 < threshold=0.5, {'title': 'Untitled', 'page_content': '4 2 0 2 2 2 v 4 2 6 2 1 . 6 0 4 2 : v i X r a 4 2 0 2 2', 'chunk_index': 0, 'metadata': {'languages': ['eng'], 'page_number': 1, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=18
Chunk is useful: {'title': 'Untitled', 'page_content': '2 v 4 2 6 2 1 . 6 0 4 2 : v i X r a', 'chunk_index': 0, 'metadata': {'languages': ['eng'], 'page_number': 1, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=18
Chunk is useful: {'title': 'Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges', 'page_content': 'Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges', 'chunk_index': 1, 'metadata': {'languages': ['eng'], 'page_number': 1, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=235
Chunk is useful: {'title': 'Abstract', 'page_content': 'Abstract Offering a promising solution to the scalability challenges associated with human evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an approach to evaluating large language models (LLMs). However, there are still many open questions about the strengths and weaknesses of this paradigm, and what potential biases it may hold. In this paper, we present a comprehensive study of the performance of various LLMs acting as judges.2 We leverage TriviaQA as a benchmark for assessing objective knowledge reasoning of LLMs and evaluate them alongside human annotations which we found to have a high inter-annotator agreement. Our study includes nine judge models and nine exam-taker models – both base and instruction-tuned. We assess the judge models’ alignment across different model sizes, families, and judge prompts. Among other results, our research rediscovers the importance of using Cohen’s kappa as a metric of alignment as opposed to simple percent agreement, showing that judges with high percent agreement can still assign vastly different scores. We find that both Llama-3 70B and GPT-4 Turbo have an excellent alignment with humans, but in terms of ranking exam taker models, they are outperformed by both JudgeLM-7B and the lexical matching method Contains, which have up to 34 points lower human alignment. Through error analysis and various other studies, including the effects of instruction length and leniency bias, we hope to provide useful lessons for using LLMs as judges in the future.', 'chunk_index': 2, 'metadata': {'languages': ['eng'], 'page_number': 1, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=236
Chunk is useful: {'title': 'Abstract', 'page_content': '1 Abstract Offering a promising solution to the scalability challenges associated with human evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an approach to evaluating large language models (LLMs). However, there are still many open questions about the strengths and weaknesses of this paradigm, and what potential biases it may hold. In this paper, we present a comprehensive study of the performance of various LLMs acting as judges.2 We leverage TriviaQA as a benchmark for assessing objective knowledge reasoning of LLMs and evaluate them alongside human annotations which we found to have a high inter-annotator agreement. Our study includes nine judge models and nine exam-taker models – both base and instruction-tuned. We assess the judge models’ alignment across different model sizes, families, and judge prompts. Among other results, our research rediscovers the importance of using Cohen’s kappa as a metric of alignment as opposed to simple percent agreement, showing that judges with high percent agreement can still assign vastly different scores. We find that both Llama-3 70B and GPT-4 Turbo have an excellent alignment with humans, but in terms of ranking exam taker models, they are outperformed by both JudgeLM-7B and the lexical matching method Contains, which have up to 34 points lower human alignment. Through error analysis and various other studies, including the effects of instruction length and leniency bias, we hope to provide useful lessons for using LLMs as judges in the future.', 'chunk_index': 2, 'metadata': {'languages': ['eng'], 'page_number': 1, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=1
Filtering out chunk, word_count=1 < threshold=10, {'title': 'Abstract', 'page_content': '1', 'chunk_index': 2, 'metadata': {'languages': ['eng'], 'page_number': 1, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=82
Chunk is useful: {'title': 'Introduction', 'page_content': 'Introduction Over the last few years, large language models (LLMs) have demonstrated remarkable capabilities across various domains (Radford et al., 2019; Brown et al., 2020; Achiam et al., 2023; AI@Meta, 2024, i.a.). As more and more new LLMs with different architectures and training methods continue to be released and their capabilities expand, accurately evaluating their performance and limitations becomes increasingly challenging (Zheng et al., 2024; Ohmer et al., 2024; Benchekroun et al., 2023; ∗Equal contribution 2Source code is available at https://github.com/UMass-Meta-LLM-Eval/llm_eval.git Introduction', 'chunk_index': 3, 'metadata': {'languages': ['eng'], 'page_number': 1, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=80
Chunk is useful: {'title': 'Introduction', 'page_content': 'Over the last few years, large language models (LLMs) have demonstrated remarkable capabilities across various domains (Radford et al., 2019; Brown et al., 2020; Achiam et al., 2023; AI@Meta, 2024, i.a.). As more and more new LLMs with different architectures and training methods continue to be released and their capabilities expand, accurately evaluating their performance and limitations becomes increasingly challenging (Zheng et al., 2024; Ohmer et al., 2024; Benchekroun et al., 2023; ∗Equal contribution 2Source code is available at https://github.com/UMass-Meta-LLM-Eval/llm_eval.git', 'chunk_index': 3, 'metadata': {'languages': ['eng'], 'page_number': 1, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=18
Chunk is useful: {'title': 'Mistral 7B', 'page_content': 'Mistral 7B Mistral 7B 123456789Rank Mistral 7B Mistral 7B 67.75 75.00 85.25 64.55 68.75 77.50 74.50 68.50 92.75', 'chunk_index': 4, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=18
Chunk is useful: {'title': 'Mistral 7B', 'page_content': 'Mistral 7B Mistral 7B 123456789Rank Mistral 7B Mistral 7B 67.75 75.00 85.25 64.55 68.75 77.50 74.50 68.50 92.75', 'chunk_index': 4, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=8
Filtering out chunk, word_count=8 < threshold=10, {'title': 'JudgeLM 7B', 'page_content': 'JudgeLM 7B JudgeLM 7B JudgeLM 7B JudgeLM 7B', 'chunk_index': 5, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=9
Filtering out chunk, word_count=9 < threshold=10, {'title': 'Llama-2 70B', 'page_content': 'Llama-2 70B Llama-2 70B Llama-2 70B Llama-2 70B 4558245301', 'chunk_index': 6, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=10
Filtering out chunk, density_ratio=0.2 < threshold=0.5, {'title': 'Llama-2 70B', 'page_content': 'Llama-2 70B Llama-2 70B Llama-2 70B Llama-2 70B Llama-2 70B', 'chunk_index': 6, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=3
Filtering out chunk, word_count=3 < threshold=10, {'title': 'Llama-2 70B', 'page_content': '4558245301 Llama-2 70B', 'chunk_index': 6, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=20
Chunk is useful: {'title': 'GPT 4Llama270B BaseLlama270B FTLlama213B BaseMistral 7BLlama2 7BBaseMistral 7BFTLlama213B FTLlama2 7BFT', 'page_content': 'GPT 4Llama270B BaseLlama270B FTLlama213B BaseMistral 7BLlama2 7BBaseMistral 7BFTLlama213B FTLlama2 7BFT GPT 4Llama270B BaseLlama270B FTLlama213B BaseMistral 7BLlama2 7BBaseMistral 7BFTLlama213B FTLlama2 7BFT', 'chunk_index': 7, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=15
Chunk is useful: {'title': 'Gemma 2B', 'page_content': 'Gemma 2B Gemma 2B Gemma 2B 67.00 74.00 84.00 100.00 44.25 75.25 100.00 79.50 99.75', 'chunk_index': 8, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=15
Chunk is useful: {'title': 'Gemma 2B', 'page_content': 'Gemma 2B Gemma 2B Gemma 2B 67.00 74.00 84.00 100.00 44.25 75.25 100.00 79.50 99.75', 'chunk_index': 8, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=16
Filtering out chunk, density_ratio=0.3125 < threshold=0.5, {'title': 'EM', 'page_content': 'EM EM EM 100% 90% 40% 50% EM EM EM EM 100% 90% 40% 50% EM', 'chunk_index': 9, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=84
Chunk is useful: {'title': 'Llama-2 13B', 'page_content': 'Llama-2 13B Llama-2 13B Human0102030405060708090100 (a) (b) Figure 1: (a) Scores assigned to all exam-taker models by the various judge models. (b) Average percent agreement (blue line) and Kappa scores (red bars) of judge models with human judges. Error bars annotate standard deviation across exam-taker models. Alignment is poor for most judge models, but both Llama3 70B and GPT-4 Turbo have Cohen’s Kappa coefficient that are indicative of excellent alignment (79 and 84, respectively), though still well below the human alignment score of 96.', 'chunk_index': 10, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=481
Chunk is useful: {'title': 'Llama-2 13B', 'page_content': 'Madaan et al., 2024). The empirical evaluation of LLMs is particularly difficult due to the diversity of their outputs and the wide range of tasks they are used for (Zhang et al., 2024; Li et al., 2023a). To evaluate LLMs, various methods have been proposed, that typically fall into one of two broad categories. Benchmarks such as MMLU (Hendrycks et al., 2021), TruthfulQA (Lin et al., 2021), and GSM8K (Cobbe et al., 2021) are used to evaluate specific capabilities of LLMs in an automated manner. Additionally, leaderboards like Chatbot Arena (Chiang et al., 2024) and Open LLM Leaderboard (Beeching et al., 2023) assign ranks to models considering pair-wise rankings of LLM outputs, done by humans or, in some cases, automated evaluation methods. Since both strategies involve evaluating free-form text responses generated by the LLMs, even in the first case, evaluating the responses is often just as challenging as generating them (see e.g. Chang et al., 2023). One proposed solution to this problem is to use multiple-choice question (MCQ) benchmarks such as MMLU, and compare the log-probabilities of the potential answers rather than evaluating the generated answer directly. However, the MCQ paradigm ly limits the range of abilities that can be evaluated, and the setup increasingly diverges from how LLMs are used in practice. Alternatively, the use of lexical matching methods such as exact match (EM) or n-gram overlap to evaluate the responses are practical and cost-efficient approaches, but are susceptible to false negatives and often fail to adequately distinguish between responses with subtle differences that change their semantic meaning. This issue is exacerbated when evaluating instruction-tuned “chat” models that are fine-tuned to carry out conversations with humans in natural language, since their responses tend to be more verbose (Saito et al., 2023; Renze and Guven, 2024). For these reasons, human evaluation remains the gold standard for evaluating LLM responses. However, human evaluation is expensive, time-consuming, and often impractical in many use cases. As a result, it has increasingly become common practice to evaluate LLM responses using another LLM as a judge model (Lin et al., 2021; Islam et al., 2023; Chiang and Lee, 2023; Liusie et al., 2024). While there are promises of alignment between LLM judges and humans (Sottana et al., 2023; Zheng et al., 2024), there are also many open questions about the strengths and weaknesses of the paradigm. In this work, we study the properties of LLMs as judges, comparing them with humans and automated evaluation methods. Contrary to prior work, we focus on a relatively ‘clean’ scenario in which inter-human agreement is as high as 96%, allowing us to distinguish ambiguity and subjectivity in the task itself from potential issues with the judge models. Using the knowledge benchmark TriviaQA (Joshi et al., 2017) as our playground, we investigate how nine different judge models with varying architectures and sizes judge nine different exam-taker models. Our main findings are:', 'chunk_index': 10, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=52
Chunk is useful: {'title': 'Llama-2 13B', 'page_content': '2 Even in relatively straightforward setups, only the best models are suitable as judges. Out of the nine judge models we considered, only GPT-4 Turbo and Llama-3 70B showed very high alignment with humans, though even those judges’ alignment is still well behind the human alignment coefficient for the task (Figure 1).', 'chunk_index': 10, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=217
Chunk is useful: {'title': 'Llama-2 13B', 'page_content': 'Cohen’s kappa distinguishes judges better than percent alignment. In some cases, high percent agreement can still give very divergent scores (Figure 2). Even Cohen’s kappa is not all telling. While GPT-4 Turbo and Llama-3 both have alignment scores that are considered excellent for discriminating different exam-taker models, their results are comparable to alternative cheaper approaches such as JudgeLM-7B and contains which have much lower alignment scores but more systematic biases (Figure 3) Through detailed error analysis, we uncover additional insights into judge performance. Improved alignment appears to be driven by improved recall rates and reduced false negatives. However, judge models struggle with under-specified answers and tend to be lenient, affecting their evaluation consistency. They are also sensitive to the length and quality of prompts. And, surprisingly, even when the judge models are asked to evaluate an answer matching verbatim with a reference answer, many judge models still sometimes fail to evaluate it correctly. Overall, our work showcases the strengths of the LLM-as-a-judge paradigm while also highlighting the need for caution against overreliance on alignment metrics, even in cases where they are high. Through error analysis, we also highlight several common failure cases that require attention. With this, we aim to contribute to a better general understanding of what is now becoming a mainstream paradigm for evaluating LLMs.', 'chunk_index': 10, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=9
Filtering out chunk, word_count=9 < threshold=10, {'title': 'Llama-2 13B', 'page_content': 'Llama-2 13B Llama-2 13B Llama-2 13B Llama-2 13B Human0102030405060708090100', 'chunk_index': 10, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=79
Chunk is useful: {'title': 'Llama-2 13B', 'page_content': '(a) (b) Figure 1: (a) Scores assigned to all exam-taker models by the various judge models. (b) Average percent agreement (blue line) and Kappa scores (red bars) of judge models with human judges. Error bars annotate standard deviation across exam-taker models. Alignment is poor for most judge models, but both Llama3 70B and GPT-4 Turbo have Cohen’s Kappa coefficient that are indicative of excellent alignment (79 and 84, respectively), though still well below the human alignment score of 96.', 'chunk_index': 10, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=481
Chunk is useful: {'title': 'Llama-2 13B', 'page_content': 'Madaan et al., 2024). The empirical evaluation of LLMs is particularly difficult due to the diversity of their outputs and the wide range of tasks they are used for (Zhang et al., 2024; Li et al., 2023a). To evaluate LLMs, various methods have been proposed, that typically fall into one of two broad categories. Benchmarks such as MMLU (Hendrycks et al., 2021), TruthfulQA (Lin et al., 2021), and GSM8K (Cobbe et al., 2021) are used to evaluate specific capabilities of LLMs in an automated manner. Additionally, leaderboards like Chatbot Arena (Chiang et al., 2024) and Open LLM Leaderboard (Beeching et al., 2023) assign ranks to models considering pair-wise rankings of LLM outputs, done by humans or, in some cases, automated evaluation methods. Since both strategies involve evaluating free-form text responses generated by the LLMs, even in the first case, evaluating the responses is often just as challenging as generating them (see e.g. Chang et al., 2023). One proposed solution to this problem is to use multiple-choice question (MCQ) benchmarks such as MMLU, and compare the log-probabilities of the potential answers rather than evaluating the generated answer directly. However, the MCQ paradigm ly limits the range of abilities that can be evaluated, and the setup increasingly diverges from how LLMs are used in practice. Alternatively, the use of lexical matching methods such as exact match (EM) or n-gram overlap to evaluate the responses are practical and cost-efficient approaches, but are susceptible to false negatives and often fail to adequately distinguish between responses with subtle differences that change their semantic meaning. This issue is exacerbated when evaluating instruction-tuned “chat” models that are fine-tuned to carry out conversations with humans in natural language, since their responses tend to be more verbose (Saito et al., 2023; Renze and Guven, 2024). For these reasons, human evaluation remains the gold standard for evaluating LLM responses. However, human evaluation is expensive, time-consuming, and often impractical in many use cases. As a result, it has increasingly become common practice to evaluate LLM responses using another LLM as a judge model (Lin et al., 2021; Islam et al., 2023; Chiang and Lee, 2023; Liusie et al., 2024). While there are promises of alignment between LLM judges and humans (Sottana et al., 2023; Zheng et al., 2024), there are also many open questions about the strengths and weaknesses of the paradigm. In this work, we study the properties of LLMs as judges, comparing them with humans and automated evaluation methods. Contrary to prior work, we focus on a relatively ‘clean’ scenario in which inter-human agreement is as high as 96%, allowing us to distinguish ambiguity and subjectivity in the task itself from potential issues with the judge models. Using the knowledge benchmark TriviaQA (Joshi et al., 2017) as our playground, we investigate how nine different judge models with varying architectures and sizes judge nine different exam-taker models. Our main findings are:', 'chunk_index': 10, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=52
Chunk is useful: {'title': 'Llama-2 13B', 'page_content': '2 Even in relatively straightforward setups, only the best models are suitable as judges. Out of the nine judge models we considered, only GPT-4 Turbo and Llama-3 70B showed very high alignment with humans, though even those judges’ alignment is still well behind the human alignment coefficient for the task (Figure 1).', 'chunk_index': 10, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=217
Chunk is useful: {'title': 'Llama-2 13B', 'page_content': 'Cohen’s kappa distinguishes judges better than percent alignment. In some cases, high percent agreement can still give very divergent scores (Figure 2). Even Cohen’s kappa is not all telling. While GPT-4 Turbo and Llama-3 both have alignment scores that are considered excellent for discriminating different exam-taker models, their results are comparable to alternative cheaper approaches such as JudgeLM-7B and contains which have much lower alignment scores but more systematic biases (Figure 3) Through detailed error analysis, we uncover additional insights into judge performance. Improved alignment appears to be driven by improved recall rates and reduced false negatives. However, judge models struggle with under-specified answers and tend to be lenient, affecting their evaluation consistency. They are also sensitive to the length and quality of prompts. And, surprisingly, even when the judge models are asked to evaluate an answer matching verbatim with a reference answer, many judge models still sometimes fail to evaluate it correctly. Overall, our work showcases the strengths of the LLM-as-a-judge paradigm while also highlighting the need for caution against overreliance on alignment metrics, even in cases where they are high. Through error analysis, we also highlight several common failure cases that require attention. With this, we aim to contribute to a better general understanding of what is now becoming a mainstream paradigm for evaluating LLMs.', 'chunk_index': 10, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=4
Filtering out chunk, word_count=4 < threshold=10, {'title': 'Llama-2 13B', 'page_content': 'Llama-2 13B Llama-2 13B', 'chunk_index': 10, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=24
Chunk is useful: {'title': 'Llama-3 70B', 'page_content': 'Llama-3 70B 0102030405060708090100 Llama-3 70B Llama-3 70B Llama-3 70B Llama-3 70B Base Llama-2 7B 46.75 Mistral 7B 59.50 Llama-2 13B 56.00 Llama-2 70B 63.75', 'chunk_index': 11, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=45
Chunk is useful: {'title': 'Llama-3 70B', 'page_content': 'Chat 24.05 20.25 0.25 36.25 Base Chat 50.75 38.98 57.25 44.00 60.00 46.25 68.00 59.50 Base 62.25 56.00 71.75 60.75 72.75 83.75 Chat Base 63.25 72.50 56.50 75.00 72.25 85.00 Chat Base 66.0 56.75 73.5 52.75 57.50 76.50 72.00 87.00 Chat 61.50 65.0 68.50 78.25', 'chunk_index': 11, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=25
Chunk is useful: {'title': 'Llama-3 70B', 'page_content': '18 Llama-3 70B 0102030405060708090100 Llama-3 70B Llama-3 70B Llama-3 70B Llama-3 70B Base Llama-2 7B 46.75 Mistral 7B 59.50 Llama-2 13B 56.00 Llama-2 70B 63.75', 'chunk_index': 11, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=45
Chunk is useful: {'title': 'Llama-3 70B', 'page_content': 'Chat 24.05 20.25 0.25 36.25 Base Chat 50.75 38.98 57.25 44.00 60.00 46.25 68.00 59.50 Base 62.25 56.00 71.75 60.75 72.75 83.75 Chat Base 63.25 72.50 56.50 75.00 72.25 85.00 Chat Base 66.0 56.75 73.5 52.75 57.50 76.50 72.00 87.00 Chat 61.50 65.0 68.50 78.25', 'chunk_index': 11, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=1
Filtering out chunk, word_count=1 < threshold=10, {'title': 'Llama-3 70B', 'page_content': '18', 'chunk_index': 11, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=4
Filtering out chunk, word_count=4 < threshold=10, {'title': 'Judges', 'page_content': 'Judges Judges Judges Judges', 'chunk_index': 12, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=4
Filtering out chunk, word_count=4 < threshold=10, {'title': 'Human', 'page_content': 'Human Human Human Human', 'chunk_index': 13, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=10
Filtering out chunk, density_ratio=0.2 < threshold=0.5, {'title': 'Llama-2 7B', 'page_content': 'Llama-2 7B Llama-2 7B Llama-2 7B Llama-2 7B Llama-2 7B', 'chunk_index': 14, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=6
Filtering out chunk, word_count=6 < threshold=10, {'title': 'Llama-2 7B', 'page_content': 'Llama-2 7B Llama-2 7B Llama-2 7B', 'chunk_index': 14, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=14
Chunk is useful: {'title': 'GPT-4', 'page_content': 'GPT-4 GPT-4 GPT-4 GPT-4 GPT-4 63.25 75.00 85.0 56.7 57.50 72.0 72.5 52.75 92.25', 'chunk_index': 15, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=14
Chunk is useful: {'title': 'GPT-4', 'page_content': 'GPT-4 GPT-4 GPT-4 GPT-4 GPT-4 63.25 75.00 85.0 56.7 57.50 72.0 72.5 52.75 92.25', 'chunk_index': 15, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=8
Filtering out chunk, word_count=8 < threshold=10, {'title': 'Contains', 'page_content': 'Contains Contains Contains Contains Contains Contains Contains Contains', 'chunk_index': 16, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=14
Chunk is useful: {'title': 'Llama-3 8B', 'page_content': 'Llama-3 8B 37 43 51 56 62 62 65 69 72 79 84 97', 'chunk_index': 17, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=18
Chunk is useful: {'title': 'Llama-3 8B', 'page_content': 'Llama-3 8B Llama-3 8B Llama-3 8B 37 43 51 56 62 62 65 69 72 79 84 97', 'chunk_index': 17, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=4
Filtering out chunk, word_count=4 < threshold=10, {'title': 'Llama-3 8B', 'page_content': 'Llama-3 8B Llama-3 8B', 'chunk_index': 17, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=2
Filtering out chunk, word_count=2 < threshold=10, {'title': 'Mistral7B', 'page_content': 'Mistral7B Mistral7B', 'chunk_index': 18, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=4
Filtering out chunk, word_count=4 < threshold=10, {'title': 'Kappa Scores', 'page_content': 'Kappa Scores Kappa Scores', 'chunk_index': 19, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=4
Filtering out chunk, word_count=4 < threshold=10, {'title': 'Percentage Agreement', 'page_content': 'Percentage Agreement Percentage Agreement', 'chunk_index': 20, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=2
Filtering out chunk, word_count=2 < threshold=10, {'title': 'JudgeLM7B', 'page_content': 'JudgeLM7B JudgeLM7B', 'chunk_index': 21, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=2
Filtering out chunk, word_count=2 < threshold=10, {'title': 'Gemma2B', 'page_content': 'Gemma2B Gemma2B', 'chunk_index': 22, 'metadata': {'languages': ['eng'], 'page_number': 2, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=387
Chunk is useful: {'title': '2 Related work', 'page_content': '2 Related work Various recent studies have used or considered using LLMs as judges for tasks such as evaluating story generation (Chiang and Lee, 2023), retrieval-augmented generation (Es et al., 2023), visual QA (Mañas et al., 2024), code comprehension (Zhiqiang et al., 2023), multilingual evaluation (Hada et al., 2023) and more general open-ended tasks (Zheng et al., 2024). Zhang et al. (2024) and Sottana et al. (2023) propose ways to standardise LLM evaluations and the role that judge models might play in such solutions. Several studies have demonstrated that state-of-the-art LLMs such as GPT-4 Turbo exhibit high alignment with human judgments (Sottana et al., 2023; Zheng et al., 2024), though others also illustrate that the paradigm is not yet without faults. Zeng et al. (2023) propose a benchmark for evaluating the performance of LLMs as judges, and other approaches have been proposed to improve LLM judges such that they are aligned well with humans (Shankar et al., 2024; Zhu et al., 2023). Despite promising results in various settings, judge models still suffer from known issues of current LLMs such as hallucinations and factual errors (Ye et al., 2023; Turpin et al., 2023) and difficulty in following complex instructions (Li et al., 2023b; He et al., 2024). Furthermore, various studies have reported challenges such as position bias (Pezeshkpour and Hruschka, 2023; Zheng et al., 2023; Wang et al., 2023), verbosity bias (Saito et al., 2023) in their preferences, confusing evaluation criteria (Hu et al., 2024), or focusing more on the style and grammar compared to factuality (Wu and Aji, 2023). Recently, Liusie et al. (2024) have shown that LLMs perform better in comparative assessment compared to absolute scoring, which can be used for reliably measuring the relative performance of models (Liu et al., 2024) and creating classifiers for pairwise grading (Huang et al., 2024). We follow up on this line of work and investigate the strengths and weaknesses of LLMs as judges. Unlike most prior work, we do not focus on pairwise comparisons of LLM outputs on open-ended tasks, but on comparisons of LLM outputs and reference answers. Since human alignment is high in this setting, this provides a clean playground to study the strengths and weaknesses of LLMs in detail. We also extend previous work by considering more LLMs, both as judges and LLMs to be evaluated.', 'chunk_index': 23, 'metadata': {'languages': ['eng'], 'page_number': 3, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=387
Chunk is useful: {'title': '2 Related work', 'page_content': '2 Related work Various recent studies have used or considered using LLMs as judges for tasks such as evaluating story generation (Chiang and Lee, 2023), retrieval-augmented generation (Es et al., 2023), visual QA (Mañas et al., 2024), code comprehension (Zhiqiang et al., 2023), multilingual evaluation (Hada et al., 2023) and more general open-ended tasks (Zheng et al., 2024). Zhang et al. (2024) and Sottana et al. (2023) propose ways to standardise LLM evaluations and the role that judge models might play in such solutions. Several studies have demonstrated that state-of-the-art LLMs such as GPT-4 Turbo exhibit high alignment with human judgments (Sottana et al., 2023; Zheng et al., 2024), though others also illustrate that the paradigm is not yet without faults. Zeng et al. (2023) propose a benchmark for evaluating the performance of LLMs as judges, and other approaches have been proposed to improve LLM judges such that they are aligned well with humans (Shankar et al., 2024; Zhu et al., 2023). Despite promising results in various settings, judge models still suffer from known issues of current LLMs such as hallucinations and factual errors (Ye et al., 2023; Turpin et al., 2023) and difficulty in following complex instructions (Li et al., 2023b; He et al., 2024). Furthermore, various studies have reported challenges such as position bias (Pezeshkpour and Hruschka, 2023; Zheng et al., 2023; Wang et al., 2023), verbosity bias (Saito et al., 2023) in their preferences, confusing evaluation criteria (Hu et al., 2024), or focusing more on the style and grammar compared to factuality (Wu and Aji, 2023). Recently, Liusie et al. (2024) have shown that LLMs perform better in comparative assessment compared to absolute scoring, which can be used for reliably measuring the relative performance of models (Liu et al., 2024) and creating classifiers for pairwise grading (Huang et al., 2024). We follow up on this line of work and investigate the strengths and weaknesses of LLMs as judges. Unlike most prior work, we do not focus on pairwise comparisons of LLM outputs on open-ended tasks, but on comparisons of LLM outputs and reference answers. Since human alignment is high in this setting, this provides a clean playground to study the strengths and weaknesses of LLMs in detail. We also extend previous work by considering more LLMs, both as judges and LLMs to be evaluated.', 'chunk_index': 23, 'metadata': {'languages': ['eng'], 'page_number': 3, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=58
Chunk is useful: {'title': '3 Methodology', 'page_content': '3 Methodology To evaluate the strengths and weaknesses of the LLM-as-a-judge paradigm, we focus on a relatively controlled setup, in which judge models assess answers of exam-taker models on the knowledge benchmark TriviaQA (Joshi et al., 2017). We consider nine judge models and nine exam-taker models. More details about the dataset and models are provided in Appendix B.', 'chunk_index': 24, 'metadata': {'languages': ['eng'], 'page_number': 3, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=498
Chunk is useful: {'title': '3 Methodology', 'page_content': '3 Llama-2 (7B, 13B, and 70B), Llama-3 (8B and 70B), Gemma 2B, Mistral 7B, JudgeLM 7B, GPT-4 Turbo Table 1: The exam-taker models and judge models we use in our experiments. We consider a wide variety of judge models; to get a comprehensive overview of their (potential) biases, we consider exam-taker models of various sizes and types. Benchmark As our testbed, we use the TriviaQA dataset (Joshi et al., 2017), consisting of 95K question-answer pairs sourced from 14 trivia and quiz league websites. Each question in the train and validation set is annotated with a list of short answers containing a minimal set of facts and evidence documents collected from Wikipedia and the Web. For our experiments, we use the validation set of the unfiltered partition of the benchmark, using the short answers as reference answers. We use the training set for few-shot examples. Since experiments require manual annotation of the exam-taker model responses, we use a random sample of 400 questions from the dataset. Exam-taker models To understand the strengths and weaknesses of different judges, we benchmark pre-trained (base) and instruction-tuned (chat) exam-taker models across a wide variety of model sizes and examine the quality of the evaluations from different judge models. In particular, we consider Llama-2 (Touvron et al., 2023) in 7B, 13B, and 70B parameter sizes for both base and chat versions, Mistral 7B (Jiang et al., 2023) base and chat versions, and GPT-4 Turbo3 (Achiam et al., 2023) as the exam-taker models. The prompts for the exam-taker models contain five few-shot examples of (question, answer) pairs from the TriviaQA training set. The prompts for the instruction-tuned models additionally include a command signaling the model to answer the given question in a succinct manner similar to the provided examples. The prompts are provided in Appendix C. Judge models To get a comprehensive view of the strengths and weaknesses of judge models across different model sizes and architectures, we use instruction-tuned versions of Llama-2 (Touvron et al., 2023) in 7B, 13B, and 70B sizes, Llama-3 (AI@Meta, 2024) in 8B and 70B sizes, Mistral 7B (Jiang et al., 2023), GPT-4 Turbo (Achiam et al., 2023), Gemma 2B (Gemma Team et al., 2024), and JudgeLM 7B (Zhu et al., 2023) as judges. The judges are instructed to respond with only a single word, “correct” or “incorrect”. The prompts can be found in Appendix D. An overview of al exam-taker models and judge models is shown in Table 1. For ease of reading, the judge models are depicted in a different font than the exam-taker models. Baselines As baselines, we use two commonly used lexical evaluation techniques – exact match (EM) and contains match (contains). For EM, a response is considered correct if the response exactly matches one of the reference answers for the given question. For contains, an answer is considered correct if at least one of the reference answers is a sub-string of the response string. Both EM and contains match are computed in a case-insensitive manner.', 'chunk_index': 24, 'metadata': {'languages': ['eng'], 'page_number': 3, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=160
Chunk is useful: {'title': '3 Methodology', 'page_content': 'Alignment We use two metrics to quantify alignment between judges: percent agreement and Cohen’s kappa coefficient (Cohen, 1960). Percent agreement expresses a simple percentage of the samples on which two annotators agree. Cohen’s kappa, denoted as κ, is an alignment metric that also takes into account the possibility of chance agreement. It is generally considered to provide a more robust measure of alignment. Details about the computation of both metrics are given in Appendix E. Human judgements As a ground-truth assessment, we obtain human annotations for each exam- taker model answer. The inter-human alignment is calculated between three human judges using the answers to 600 questions by Llama-2 7B base, randomly sampled from the TriviaQA dataset (Joshi et al., 2017); the human guidelines can be found in Appendix F. We then determine collective “Human Judgment” through a majority vote. We found that the average alignment among human 3Accessed via the OpenAI API between Mar 19th, 2024 and May 19th, 2024.', 'chunk_index': 24, 'metadata': {'languages': ['eng'], 'page_number': 3, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=70
Chunk is useful: {'title': '3 Methodology', 'page_content': "4 10 60 100 20 1 No Agreement 1 100Cohen's Kappa 40 3 Methodology To evaluate the strengths and weaknesses of the LLM-as-a-judge paradigm, we focus on a relatively controlled setup, in which judge models assess answers of exam-taker models on the knowledge benchmark TriviaQA (Joshi et al., 2017). We consider nine judge models and nine exam-taker models. More details about the dataset and models are provided in Appendix B.", 'chunk_index': 24, 'metadata': {'languages': ['eng'], 'page_number': 3, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=498
Chunk is useful: {'title': '3 Methodology', 'page_content': '3 Llama-2 (7B, 13B, and 70B), Llama-3 (8B and 70B), Gemma 2B, Mistral 7B, JudgeLM 7B, GPT-4 Turbo Table 1: The exam-taker models and judge models we use in our experiments. We consider a wide variety of judge models; to get a comprehensive overview of their (potential) biases, we consider exam-taker models of various sizes and types. Benchmark As our testbed, we use the TriviaQA dataset (Joshi et al., 2017), consisting of 95K question-answer pairs sourced from 14 trivia and quiz league websites. Each question in the train and validation set is annotated with a list of short answers containing a minimal set of facts and evidence documents collected from Wikipedia and the Web. For our experiments, we use the validation set of the unfiltered partition of the benchmark, using the short answers as reference answers. We use the training set for few-shot examples. Since experiments require manual annotation of the exam-taker model responses, we use a random sample of 400 questions from the dataset. Exam-taker models To understand the strengths and weaknesses of different judges, we benchmark pre-trained (base) and instruction-tuned (chat) exam-taker models across a wide variety of model sizes and examine the quality of the evaluations from different judge models. In particular, we consider Llama-2 (Touvron et al., 2023) in 7B, 13B, and 70B parameter sizes for both base and chat versions, Mistral 7B (Jiang et al., 2023) base and chat versions, and GPT-4 Turbo3 (Achiam et al., 2023) as the exam-taker models. The prompts for the exam-taker models contain five few-shot examples of (question, answer) pairs from the TriviaQA training set. The prompts for the instruction-tuned models additionally include a command signaling the model to answer the given question in a succinct manner similar to the provided examples. The prompts are provided in Appendix C. Judge models To get a comprehensive view of the strengths and weaknesses of judge models across different model sizes and architectures, we use instruction-tuned versions of Llama-2 (Touvron et al., 2023) in 7B, 13B, and 70B sizes, Llama-3 (AI@Meta, 2024) in 8B and 70B sizes, Mistral 7B (Jiang et al., 2023), GPT-4 Turbo (Achiam et al., 2023), Gemma 2B (Gemma Team et al., 2024), and JudgeLM 7B (Zhu et al., 2023) as judges. The judges are instructed to respond with only a single word, “correct” or “incorrect”. The prompts can be found in Appendix D. An overview of al exam-taker models and judge models is shown in Table 1. For ease of reading, the judge models are depicted in a different font than the exam-taker models. Baselines As baselines, we use two commonly used lexical evaluation techniques – exact match (EM) and contains match (contains). For EM, a response is considered correct if the response exactly matches one of the reference answers for the given question. For contains, an answer is considered correct if at least one of the reference answers is a sub-string of the response string. Both EM and contains match are computed in a case-insensitive manner.', 'chunk_index': 24, 'metadata': {'languages': ['eng'], 'page_number': 3, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=160
Chunk is useful: {'title': '3 Methodology', 'page_content': 'Alignment We use two metrics to quantify alignment between judges: percent agreement and Cohen’s kappa coefficient (Cohen, 1960). Percent agreement expresses a simple percentage of the samples on which two annotators agree. Cohen’s kappa, denoted as κ, is an alignment metric that also takes into account the possibility of chance agreement. It is generally considered to provide a more robust measure of alignment. Details about the computation of both metrics are given in Appendix E. Human judgements As a ground-truth assessment, we obtain human annotations for each exam- taker model answer. The inter-human alignment is calculated between three human judges using the answers to 600 questions by Llama-2 7B base, randomly sampled from the TriviaQA dataset (Joshi et al., 2017); the human guidelines can be found in Appendix F. We then determine collective “Human Judgment” through a majority vote. We found that the average alignment among human 3Accessed via the OpenAI API between Mar 19th, 2024 and May 19th, 2024.', 'chunk_index': 24, 'metadata': {'languages': ['eng'], 'page_number': 3, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=12
Chunk is useful: {'title': '3 Methodology', 'page_content': "4 10 60 100 20 1 No Agreement 1 100Cohen's Kappa 40", 'chunk_index': 24, 'metadata': {'languages': ['eng'], 'page_number': 3, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=25
Chunk is useful: {'title': '100Percentage Agreement', 'page_content': "100Percentage Agreement 10 80 100 0 100 40 60 1 No Agreement 80 10 100Percentage Agreement 20 1 100Cohen's Kappa 100 10 0 (a) (b)", 'chunk_index': 25, 'metadata': {'languages': ['eng'], 'page_number': 5, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=132
Chunk is useful: {'title': '100Percentage Agreement', 'page_content': 'Figure 2: Delta evaluation score is calculated by taking judge score difference with human judgment; y-axes are in log scale. Fig (a) shows a skewed distribution for percent agreement and delta evaluation score. In Fig (b) we observe that highly aligned LLM judges with kappa > 0.8 exhibit low variability in scores. Conversely, judges with kappa < 0.8 demonstrate more variability, impacting their reliability. evaluators with the majority vote had a Cohen’s kappa score4 of 96.36 ± 1.67, and the average percent agreement was 98.33% ± 0.76%. Given this near-perfect alignment score, we consider only one human evaluator per sample for the rest of our experiments, to reduce the overall cost of human annotations. The set of questions for which we obtain human annotations is identical for each exam-taker model. 100Percentage Agreement', 'chunk_index': 25, 'metadata': {'languages': ['eng'], 'page_number': 5, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=23
Chunk is useful: {'title': '100Percentage Agreement', 'page_content': "10 80 100 0 100 40 60 1 No Agreement 80 10 100Percentage Agreement 20 1 100Cohen's Kappa 100 10 0 (a) (b)", 'chunk_index': 25, 'metadata': {'languages': ['eng'], 'page_number': 5, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=130
Chunk is useful: {'title': '100Percentage Agreement', 'page_content': 'Figure 2: Delta evaluation score is calculated by taking judge score difference with human judgment; y-axes are in log scale. Fig (a) shows a skewed distribution for percent agreement and delta evaluation score. In Fig (b) we observe that highly aligned LLM judges with kappa > 0.8 exhibit low variability in scores. Conversely, judges with kappa < 0.8 demonstrate more variability, impacting their reliability. evaluators with the majority vote had a Cohen’s kappa score4 of 96.36 ± 1.67, and the average percent agreement was 98.33% ± 0.76%. Given this near-perfect alignment score, we consider only one human evaluator per sample for the rest of our experiments, to reduce the overall cost of human annotations. The set of questions for which we obtain human annotations is identical for each exam-taker model.', 'chunk_index': 25, 'metadata': {'languages': ['eng'], 'page_number': 5, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=192
Filtering out chunk, density_ratio=0.3854166666666667 < threshold=0.5, {'title': '4 Results', 'page_content': '4 Results In this section we discuss our main results, primarily focusing on the relationship between evaluations by various judge models and human evaluations (§ 4.1), and how that impacts their usability (§ 4.2). To do so, we evaluate their alignment with human judgment and assess how differently they rank the nine exam-taker models compared to humans. In Section 5, we further analyse their precision and recall to further investigate the types of errors that can be made by various judge models. Details about compute requirements and others costs for experiments are given in Appendix G. 4 Results In this section we discuss our main results, primarily focusing on the relationship between evaluations by various judge models and human evaluations (§ 4.1), and how that impacts their usability (§ 4.2). To do so, we evaluate their alignment with human judgment and assess how differently they rank the nine exam-taker models compared to humans. In Section 5, we further analyse their precision and recall to further investigate the types of errors that can be made by various judge models. Details about compute requirements and others costs for experiments are given in Appendix G.', 'chunk_index': 26, 'metadata': {'languages': ['eng'], 'page_number': 5, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=254
Chunk is useful: {'title': '4.1 Alignment between judge models and humans', 'page_content': '4.1 Alignment between judge models and humans We first compute κ scores and percent agreement between the evaluations of each judge model and the human annotators. Figure 1 shows that while alignment is poor for most judge models, both Llama-3 70B and GPT-4 Turbo have κ scores – 79 and 84, respectively – that are considered to indicate excellent alignment. Nevertheless, there still is a significant disparity between human judgment and judge models: GPT-4 Turbo is 12 points behind human judgment. Notably, contains has a higher κ score than half of the judge models, while EM has the lowest alignment among all judges. Cohen’s kappa vs percent agreement In most cases, we observe that Cohen’s kappa and percent agreement are following the same trend, with the exception of the values for Gemma 2B and EM. Gemma 2B shows higher percent agreement compared to EM, yet it yields the lowest κ score within the ensemble. Furthermore, there is a significant difference in the actual values. For the percent agreement of judge models, we note a 30-point difference between human judgment and EM, while Cohen’s kappa exhibits a more substantial 53-point gap. This is also visible in the general decline of alignment scores: while Llama-3 8B has a κ score of only 62, its percent agreement is still well above 80%. Overall, κ scores offer a more precise representation of diminishing trends in judge models compared to percent agreement. 4The values of the Cohen’s kappa coefficient have been scaled by 100× for easier comparison with percent', 'chunk_index': 27, 'metadata': {'languages': ['eng'], 'page_number': 5, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=255
Chunk is useful: {'title': '4.1 Alignment between judge models and humans', 'page_content': '5 4.1 Alignment between judge models and humans We first compute κ scores and percent agreement between the evaluations of each judge model and the human annotators. Figure 1 shows that while alignment is poor for most judge models, both Llama-3 70B and GPT-4 Turbo have κ scores – 79 and 84, respectively – that are considered to indicate excellent alignment. Nevertheless, there still is a significant disparity between human judgment and judge models: GPT-4 Turbo is 12 points behind human judgment. Notably, contains has a higher κ score than half of the judge models, while EM has the lowest alignment among all judges. Cohen’s kappa vs percent agreement In most cases, we observe that Cohen’s kappa and percent agreement are following the same trend, with the exception of the values for Gemma 2B and EM. Gemma 2B shows higher percent agreement compared to EM, yet it yields the lowest κ score within the ensemble. Furthermore, there is a significant difference in the actual values. For the percent agreement of judge models, we note a 30-point difference between human judgment and EM, while Cohen’s kappa exhibits a more substantial 53-point gap. This is also visible in the general decline of alignment scores: while Llama-3 8B has a κ score of only 62, its percent agreement is still well above 80%. Overall, κ scores offer a more precise representation of diminishing trends in judge models compared to percent agreement. 4The values of the Cohen’s kappa coefficient have been scaled by 100× for easier comparison with percent', 'chunk_index': 27, 'metadata': {'languages': ['eng'], 'page_number': 5, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=1
Filtering out chunk, word_count=1 < threshold=10, {'title': '4.1 Alignment between judge models and humans', 'page_content': '5', 'chunk_index': 27, 'metadata': {'languages': ['eng'], 'page_number': 5, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=6
Filtering out chunk, word_count=6 < threshold=10, {'title': 'Llama-2 13B Base', 'page_content': 'Llama-2 13B Base Llama-2 13B Base', 'chunk_index': 28, 'metadata': {'languages': ['eng'], 'page_number': 6, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=6
Filtering out chunk, word_count=6 < threshold=10, {'title': 'Exam Taker Models', 'page_content': 'Exam Taker Models Exam Taker Models', 'chunk_index': 29, 'metadata': {'languages': ['eng'], 'page_number': 6, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=6
Filtering out chunk, word_count=6 < threshold=10, {'title': 'Llama-2 70B Base', 'page_content': 'Llama-2 70B Base Llama-2 70B Base', 'chunk_index': 30, 'metadata': {'languages': ['eng'], 'page_number': 6, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=6
Filtering out chunk, word_count=6 < threshold=10, {'title': 'Mistral 7B FT', 'page_content': 'Mistral 7B FT Mistral 7B FT', 'chunk_index': 31, 'metadata': {'languages': ['eng'], 'page_number': 6, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=2
Filtering out chunk, word_count=2 < threshold=10, {'title': 'HumanContainsJudgeLM7BGPT-4Llama-370BEM', 'page_content': 'HumanContainsJudgeLM7BGPT-4Llama-370BEM HumanContainsJudgeLM7BGPT-4Llama-370BEM', 'chunk_index': 32, 'metadata': {'languages': ['eng'], 'page_number': 6, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=6
Filtering out chunk, word_count=6 < threshold=10, {'title': 'Llama-2 7B FT', 'page_content': 'Llama-2 7B FT Llama-2 7B FT', 'chunk_index': 33, 'metadata': {'languages': ['eng'], 'page_number': 6, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=4
Filtering out chunk, word_count=4 < threshold=10, {'title': 'GPT 4', 'page_content': 'GPT 4 GPT 4', 'chunk_index': 34, 'metadata': {'languages': ['eng'], 'page_number': 6, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=6
Filtering out chunk, word_count=6 < threshold=10, {'title': 'Llama-2 13B FT', 'page_content': 'Llama-2 13B FT Llama-2 13B FT', 'chunk_index': 35, 'metadata': {'languages': ['eng'], 'page_number': 6, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=6
Filtering out chunk, word_count=6 < threshold=10, {'title': 'Llama-2 7B Base', 'page_content': 'Llama-2 7B Base Llama-2 7B Base', 'chunk_index': 36, 'metadata': {'languages': ['eng'], 'page_number': 6, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=6
Filtering out chunk, word_count=6 < threshold=10, {'title': 'Llama-2 70B FT', 'page_content': 'Llama-2 70B FT Llama-2 70B FT', 'chunk_index': 37, 'metadata': {'languages': ['eng'], 'page_number': 6, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=6
Filtering out chunk, word_count=6 < threshold=10, {'title': 'True Negative', 'page_content': 'True Negative 16.76%68.23%55.07%66.98%54.46%63.66%62.85%51.27%14.74%20.17%15.09%48.61%18.28%20.94%63.19%31.33%27.73%24.01%38.22%22.41%30.20%21.75%21.50%28.58%28.20%15.71%30.45%66.60%6.45%8.00%8.04%8.70%8.95%1.88%2.25%6.70%6.35%2.72% True Negative 16.76%68.23%55.07%66.98%54.46%63.66%62.85%51.27%14.74%20.17%15.09%48.61%18.28%20.94%63.19%31.33%27.73%24.01%38.22%22.41%30.20%21.75%21.50%28.58%28.20%15.71%30.45%66.60%6.45%8.00%8.04%8.70%8.95%1.88%2.25%6.70%6.35%2.72%', 'chunk_index': 38, 'metadata': {'languages': ['eng'], 'page_number': 6, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=4
Filtering out chunk, word_count=4 < threshold=10, {'title': 'False Positive', 'page_content': 'False Positive False Positive', 'chunk_index': 39, 'metadata': {'languages': ['eng'], 'page_number': 6, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=4
Filtering out chunk, word_count=4 < threshold=10, {'title': 'False Negative', 'page_content': 'False Negative False Negative', 'chunk_index': 40, 'metadata': {'languages': ['eng'], 'page_number': 6, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=214
Chunk is useful: {'title': 'True Positive', 'page_content': 'True Positive (a) (b) Figure 3: (a) Contains and JudgeLM retain 67% of the human-assigned ranking, closely followed by GPT-4 Turbo and LLama3-70B. All judges struggle to distinguish between the poor-performing exam-taker models; (b) False positives and negatives across different judge models in descending order of human alignment. Both false negatives and false positives generally increase as human alignment decreases, but well-aligned models tend to produce more false negatives than false positives. Alignment vs assigned score In Figure 2, we show the variation in scores assigned by the judge models to various exam-taker models for different values of percent agreement (Figure 2a) and Cohen’s kappa (Figure 2b). We can see that for κ > 80, the evaluation scores by judge models are close to the human evaluation scores for most of the judges, with a difference of up to 5 points in their assigned scores (complete results table provided in Appendix H). For percent agreement we observe deviations of up to 20 points in the evaluation scores for similar kappa alignment. Furthermore, we observe that the deviation from human-judgements can be quite distinct for different exam-taker models. In Figure 1a, Gemma 2B, for instance, sometimes assigns higher scores than humans, and sometimes much lower. In the next section, we further explore this particular pattern.', 'chunk_index': 41, 'metadata': {'languages': ['eng'], 'page_number': 6, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=214
Chunk is useful: {'title': 'True Positive', 'page_content': 'True Positive (a) (b) Figure 3: (a) Contains and JudgeLM retain 67% of the human-assigned ranking, closely followed by GPT-4 Turbo and LLama3-70B. All judges struggle to distinguish between the poor-performing exam-taker models; (b) False positives and negatives across different judge models in descending order of human alignment. Both false negatives and false positives generally increase as human alignment decreases, but well-aligned models tend to produce more false negatives than false positives. Alignment vs assigned score In Figure 2, we show the variation in scores assigned by the judge models to various exam-taker models for different values of percent agreement (Figure 2a) and Cohen’s kappa (Figure 2b). We can see that for κ > 80, the evaluation scores by judge models are close to the human evaluation scores for most of the judges, with a difference of up to 5 points in their assigned scores (complete results table provided in Appendix H). For percent agreement we observe deviations of up to 20 points in the evaluation scores for similar kappa alignment. Furthermore, we observe that the deviation from human-judgements can be quite distinct for different exam-taker models. In Figure 1a, Gemma 2B, for instance, sometimes assigns higher scores than humans, and sometimes much lower. In the next section, we further explore this particular pattern.', 'chunk_index': 41, 'metadata': {'languages': ['eng'], 'page_number': 6, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=296
Chunk is useful: {'title': '4.2 Exploring systematic patterns in judge models', 'page_content': '4.2 Exploring systematic patterns in judge models In the previous section, we have seen that none of the judge models we considered were aligned as well with humans as the humans themselves. Furthermore, as can be seen in Figure 2, the scores assigned by even the best aligned judge models can differ up to 10 points with the human-assigned scores. However, while this may limit – to some extent – the utility of using a judge models to get a perfect estimate of the exam-taker model’s capability on the benchmark, the judge models may still offer valuable insights to differentiate between different exam-taker models. If judges exhibit systematic biases such as – akin to a very strict teacher – consistently rating any exam-taker model lower, they will not assign identical scores but may assign identical rankings. To evaluate this, we compare the rankings assigned by each judge model to the nine exam-taker models by computing their Spearman’s rank correlation coefficients ρ (Spearman, 1904) with the human ranking. The rankings are shown in Figure 3a, with ρ values in Appendix J. These results show that contains demonstrates the highest alignment with the human ranking, swapping the ranks of only two out of nine models. Notably, contains performs on par with JudgeLM 7B (Zhu et al., 2023), a language model fine-tuned specifically for evaluating language model responses. They are closely followed by GPT-4 Turbo and Llama-3 70B, the judges with the best alignment. Remarkably, while GPT-4 Turbo and Llama-3 70B rank the same three models in positions three, four, and five as the human judge, they do so in different orders. Several other judges have rank correlations higher than 0.7; it appears they struggle to distinguish between poorer-performing exam-taker models, but do well at distinguishing between better-performing ones.', 'chunk_index': 42, 'metadata': {'languages': ['eng'], 'page_number': 6, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=296
Chunk is useful: {'title': '4.2 Exploring systematic patterns in judge models', 'page_content': '4.2 Exploring systematic patterns in judge models In the previous section, we have seen that none of the judge models we considered were aligned as well with humans as the humans themselves. Furthermore, as can be seen in Figure 2, the scores assigned by even the best aligned judge models can differ up to 10 points with the human-assigned scores. However, while this may limit – to some extent – the utility of using a judge models to get a perfect estimate of the exam-taker model’s capability on the benchmark, the judge models may still offer valuable insights to differentiate between different exam-taker models. If judges exhibit systematic biases such as – akin to a very strict teacher – consistently rating any exam-taker model lower, they will not assign identical scores but may assign identical rankings. To evaluate this, we compare the rankings assigned by each judge model to the nine exam-taker models by computing their Spearman’s rank correlation coefficients ρ (Spearman, 1904) with the human ranking. The rankings are shown in Figure 3a, with ρ values in Appendix J. These results show that contains demonstrates the highest alignment with the human ranking, swapping the ranks of only two out of nine models. Notably, contains performs on par with JudgeLM 7B (Zhu et al., 2023), a language model fine-tuned specifically for evaluating language model responses. They are closely followed by GPT-4 Turbo and Llama-3 70B, the judges with the best alignment. Remarkably, while GPT-4 Turbo and Llama-3 70B rank the same three models in positions three, four, and five as the human judge, they do so in different orders. Several other judges have rank correlations higher than 0.7; it appears they struggle to distinguish between poorer-performing exam-taker models, but do well at distinguishing between better-performing ones.', 'chunk_index': 42, 'metadata': {'languages': ['eng'], 'page_number': 6, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=42
Chunk is useful: {'title': '5 Analysis', 'page_content': '5 Analysis To better understand the judge models, we conduct multiple case studies aimed at identifying common errors and vulnerabilities. 6 70% 5 Analysis To better understand the judge models, we conduct multiple case studies aimed at identifying common errors and vulnerabilities.', 'chunk_index': 43, 'metadata': {'languages': ['eng'], 'page_number': 6, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=2
Filtering out chunk, word_count=2 < threshold=10, {'title': '5 Analysis', 'page_content': '6 70%', 'chunk_index': 43, 'metadata': {'languages': ['eng'], 'page_number': 6, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=2
Filtering out chunk, word_count=2 < threshold=10, {'title': 'Precision', 'page_content': 'Precision Precision', 'chunk_index': 44, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=2
Filtering out chunk, word_count=2 < threshold=10, {'title': 'Recall', 'page_content': 'Recall Recall', 'chunk_index': 45, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=2
Filtering out chunk, word_count=2 < threshold=10, {'title': 'GPT-4Judges', 'page_content': 'GPT-4Judges GPT-4Judges', 'chunk_index': 46, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=10
Chunk is useful: {'title': 'Judge Kappa Score', 'page_content': 'Judge Kappa Score 60% 80% Judge Kappa Score 60% 80%', 'chunk_index': 47, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=6
Filtering out chunk, word_count=6 < threshold=10, {'title': 'EMWithoutGuidelines V1WithoutGuidelines V2GuidelinesWithoutExamplesGuidelinesWithExamples', 'page_content': 'EMWithoutGuidelines V1WithoutGuidelines V2GuidelinesWithoutExamplesGuidelinesWithExamples EMWithoutGuidelines V1WithoutGuidelines V2GuidelinesWithoutExamplesGuidelinesWithExamples', 'chunk_index': 48, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=6
Filtering out chunk, word_count=6 < threshold=10, {'title': 'Mistral', 'page_content': "Mistral 0.10.20.30.40.50.60.70.80.9Cohen's Kappa κ (a) (b)", 'chunk_index': 49, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=256
Chunk is useful: {'title': 'Mistral', 'page_content': 'Figure 4: (a) Cohen’s kappa, precision, and recall for different judge models. Recall improves with increasing human alignment (R2 = 0.98), though precision (R2 = 0.0003) is not correlated with human alignment; (b) κ scores for each judge across different prompt templates. Except for GPT-4 Turbo, all judges struggle with too many detailed evaluation guidelines. 5.1 Better aligned models: Recall gains, precision pains, and error spotlights We first investigate the precision and recall of the judge models. We plot both – maintaining the ordering of Figure 1 – in Figure 4a. It can be seen that the precision does not show any clear observable trend with increasing alignment, which can be further observed in Figure 3b. The recall, on the other hand, shows an increasing trend, with more aligned models having comparatively fewer false negatives. Next, we analyse the types of errors made by the judge models by manually annotating 900 outputs from Llama-7B Base with error codes, focusing on the top performers GPT-4 Turbo and Llama-3 70B. We then determine the percentage of each error type that are correctly judged to be incorrect by these two models. The results are shown in Table 2, where it can be observed that both GPT-4 Turbo and Llama-3 70B have a good error recall when the answers refer to an incorrect entity, or when too many entities are present. Under-specified and otherwise incorrect answers are most challenging for both judges, while answers with too few entities are judged relatively accurately by GPT-4 but less accurately by Llama-3 70B.', 'chunk_index': 49, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=5
Filtering out chunk, word_count=5 < threshold=10, {'title': 'Mistral', 'page_content': "Mistral Mistral 0.10.20.30.40.50.60.70.80.9Cohen's Kappa κ", 'chunk_index': 49, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=258
Chunk is useful: {'title': 'Mistral', 'page_content': '(a) (b) Figure 4: (a) Cohen’s kappa, precision, and recall for different judge models. Recall improves with increasing human alignment (R2 = 0.98), though precision (R2 = 0.0003) is not correlated with human alignment; (b) κ scores for each judge across different prompt templates. Except for GPT-4 Turbo, all judges struggle with too many detailed evaluation guidelines. 5.1 Better aligned models: Recall gains, precision pains, and error spotlights We first investigate the precision and recall of the judge models. We plot both – maintaining the ordering of Figure 1 – in Figure 4a. It can be seen that the precision does not show any clear observable trend with increasing alignment, which can be further observed in Figure 3b. The recall, on the other hand, shows an increasing trend, with more aligned models having comparatively fewer false negatives. Next, we analyse the types of errors made by the judge models by manually annotating 900 outputs from Llama-7B Base with error codes, focusing on the top performers GPT-4 Turbo and Llama-3 70B. We then determine the percentage of each error type that are correctly judged to be incorrect by these two models. The results are shown in Table 2, where it can be observed that both GPT-4 Turbo and Llama-3 70B have a good error recall when the answers refer to an incorrect entity, or when too many entities are present. Under-specified and otherwise incorrect answers are most challenging for both judges, while answers with too few entities are judged relatively accurately by GPT-4 but less accurately by Llama-3 70B.', 'chunk_index': 49, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=1
Filtering out chunk, word_count=1 < threshold=10, {'title': 'Mistral', 'page_content': 'Mistral', 'chunk_index': 49, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=4
Filtering out chunk, word_count=4 < threshold=10, {'title': 'Error code', 'page_content': 'Error code Error code', 'chunk_index': 50, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=2
Filtering out chunk, word_count=2 < threshold=10, {'title': 'Explanation', 'page_content': 'Explanation Explanation', 'chunk_index': 51, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=2
Filtering out chunk, word_count=2 < threshold=10, {'title': 'Example', 'page_content': 'Example Example', 'chunk_index': 52, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=12
Filtering out chunk, density_ratio=0.4166666666666667 < threshold=0.5, {'title': 'Proportion GPT-4 recall Llama-3 70B recall', 'page_content': 'Proportion GPT-4 recall Llama-3 70B recall Proportion GPT-4 recall Llama-3 70B recall', 'chunk_index': 53, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=4
Filtering out chunk, word_count=4 < threshold=10, {'title': 'Incorrect entity', 'page_content': 'Incorrect entity Incorrect entity', 'chunk_index': 54, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=12
Chunk is useful: {'title': 'Response refers to a wrong entity', 'page_content': 'Response refers to a wrong entity Response refers to a wrong entity', 'chunk_index': 55, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=16
Chunk is useful: {'title': 'Under-specified', 'page_content': 'Under-specified Response contains only part of the answer Under-specified Response contains only part of the answer', 'chunk_index': 56, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=32
Filtering out chunk, density_ratio=0.21875 < threshold=0.5, {'title': 'Too few entities', 'page_content': 'Too few entities Response contains too few entities Too many entities Response contains too many entities Too few entities Response contains too few entities Too many entities Response contains too many entities', 'chunk_index': 57, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=67
Chunk is useful: {'title': 'Other', 'page_content': 'Other Response is incorrect but cannot be put into any of the above buckets Henry VII, James I, Edward VI, Mary I and Elizabeth I Henry VII, Henry VIII, Edward, Mary, and Elizabeth Henry VII, Edward VI, Mary I and James I Henry VII, Henry VIII, Edward VI, Mary I, James I, and Elizabeth I I’m sorry but I do not know the answer to that question', 'chunk_index': 58, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=15
Chunk is useful: {'title': 'Other', 'page_content': '86.9% 37.3% 2.47% 2.7% 1.23% 98.3% 33.9% 80.0% 90.1% 20.0% 96.6% 23.3% 60.0% 90.1% 40.0%', 'chunk_index': 58, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=57
Chunk is useful: {'title': 'Other', 'page_content': 'Table 2: Error codes used to identify the types of errors made by exam-taker models when answering questions. The example question in this case is “Excluding Lady Jane Grey, who were the five monarchs of the House of Tudor?”, with the correct answer “Henry VII, Henry VIII, Edward VI, Mary I and Elizabeth I” (in any order).', 'chunk_index': 58, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=76
Chunk is useful: {'title': 'Other', 'page_content': '5.2 Judge model sensitivity to prompt length and specificity Next, we study the impact of the prompt on the ability of the judge models to perform an accurate assessment of exam-taker models, with two somewhat intertwined goals: 1) to study if the success of various judge models is related to the length of the prompt, and, 2) to study the degree to which the judgments of the judge models change with the specificity of the prompt.', 'chunk_index': 58, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=68
Chunk is useful: {'title': 'Other', 'page_content': '7 Other Response is incorrect but cannot be put into any of the above buckets Henry VII, James I, Edward VI, Mary I and Elizabeth I Henry VII, Henry VIII, Edward, Mary, and Elizabeth Henry VII, Edward VI, Mary I and James I Henry VII, Henry VIII, Edward VI, Mary I, James I, and Elizabeth I I’m sorry but I do not know the answer to that question', 'chunk_index': 58, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=15
Chunk is useful: {'title': 'Other', 'page_content': '86.9% 37.3% 2.47% 2.7% 1.23% 98.3% 33.9% 80.0% 90.1% 20.0% 96.6% 23.3% 60.0% 90.1% 40.0%', 'chunk_index': 58, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=57
Chunk is useful: {'title': 'Other', 'page_content': 'Table 2: Error codes used to identify the types of errors made by exam-taker models when answering questions. The example question in this case is “Excluding Lady Jane Grey, who were the five monarchs of the House of Tudor?”, with the correct answer “Henry VII, Henry VIII, Edward VI, Mary I and Elizabeth I” (in any order).', 'chunk_index': 58, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=76
Chunk is useful: {'title': 'Other', 'page_content': '5.2 Judge model sensitivity to prompt length and specificity Next, we study the impact of the prompt on the ability of the judge models to perform an accurate assessment of exam-taker models, with two somewhat intertwined goals: 1) to study if the success of various judge models is related to the length of the prompt, and, 2) to study the degree to which the judgments of the judge models change with the specificity of the prompt.', 'chunk_index': 58, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=1
Filtering out chunk, word_count=1 < threshold=10, {'title': 'Other', 'page_content': '7', 'chunk_index': 58, 'metadata': {'languages': ['eng'], 'page_number': 7, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=4
Filtering out chunk, word_count=4 < threshold=10, {'title': 'Gold Answer', 'page_content': 'Gold Answer Gold Answer', 'chunk_index': 59, 'metadata': {'languages': ['eng'], 'page_number': 8, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=2
Filtering out chunk, word_count=2 < threshold=10, {'title': 'Repeater', 'page_content': 'Repeater Repeater', 'chunk_index': 60, 'metadata': {'languages': ['eng'], 'page_number': 8, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=6
Filtering out chunk, word_count=6 < threshold=10, {'title': '020406080100Percent of questions', 'page_content': '020406080100Percent of questions 020406080100Percent of questions', 'chunk_index': 61, 'metadata': {'languages': ['eng'], 'page_number': 8, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=2
Filtering out chunk, word_count=2 < threshold=10, {'title': 'Gemma2BLlama-213BJudgeLM7BLlama-38BLlama-27BMistral7BLlama-270BLlama-370BGPT-4', 'page_content': 'Gemma2BLlama-213BJudgeLM7BLlama-38BLlama-27BMistral7BLlama-270BLlama-370BGPT-4 Gemma2BLlama-213BJudgeLM7BLlama-38BLlama-27BMistral7BLlama-270BLlama-370BGPT-4', 'chunk_index': 62, 'metadata': {'languages': ['eng'], 'page_number': 8, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=2
Filtering out chunk, word_count=2 < threshold=10, {'title': 'Yes', 'page_content': 'Yes Yes', 'chunk_index': 63, 'metadata': {'languages': ['eng'], 'page_number': 8, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=327
Chunk is useful: {'title': 'Sure', 'page_content': 'Sure Figure 5: We observe that judge models remain robust when exam-taker models produce responses identical to the prompt. However, this robustness diminishes in the presence of hallucinated responses such as "Yes" and "Sure" from the exam-taker models. Additionally, when assessing the gold standard answer, judges do not consistently arrive at the correct judgement 100% of the time. We use four different prompt versions, varying in length and specificity. Each prompt instructs the judge models on how to evaluate responses, with complexity and detail increasing with the token count. The first two prompts (Without guidelines V1 and V2, with 45 and 58 tokens, respectively) simply ask to evaluate the responses, without any further information, while more elaborate guidance and examples are given in the longer prompts (Guidelines without examples and Guidelines with examples, with 245 and 301 tokens, respectively). All prompts are listed in Appendix K. Figure 4b shows that Mistral 7B (Jiang et al., 2023), GPT-4 Turbo and Llama-3 70B exhibit relatively low variance in their agreement with humans as the level of information and the length of the prompt increases. For this task, top performers’ (GPT-4 Turbo and Llama-3 70B) implicit definition of a correct judgment seems well aligned with the provided instructions and thus shows high alignment with humans even if no specific instructions are provided. It can also be observed that only top performers appears to benefit from the more detailed instructions, with a slight upward trend, whereas the other models get less aligned with more instructions. This might be due to the less powerful judges not being able to follow many instructions in the prompt at the same time. Interestingly, in a follow-up experiment (see Appendix L), when the order of references provided to the judge models is shuffled, Figure 6b demonstrates that larger judge models consistently maintain their judgments regardless of the reference order, whereas smaller models except Mistral 7B are more sensitive to the reference order given in the prompt.', 'chunk_index': 64, 'metadata': {'languages': ['eng'], 'page_number': 8, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=270
Chunk is useful: {'title': 'Sure', 'page_content': '5.3 Evaluating controlled responses Next, we perform some simple tests on the judge models by asking them to evaluate a set of dummy benchmark responses. For the first test, the answer to be evaluated for each question is one of the references from the dataset, verbatinm (the answer is thus always correct), while for the next three tests, the answer to be evaluated is always incorrect, with the dummy exam-taker model always responding with “Yes”, and “Sure” for the second and the third tests, respectively, and simply repeating the question for the fourth test. Figure 5 shows that while some judge models are able to identify and correctly mark the answers as correct (for the first test) or incorrect (for the next three tests), some judges, notably Llama-2 70B incorrectly evaluates a significant number of dummy answers, even though it shows a relatively high alignment with humans on the benchmark evaluations (see Figure 1b). We hypothesise that when the answers are plausible but incorrect (e.g. if the question asks about the name of the author of a book, and the exam-taker model gives the name of the wrong author), most judges are able to identify them as being incorrect (by comparing it with the reference answer). However, the judges might get confused about what they are supposed to evaluate if the answer is completely unrelated to the question (such as the words “Yes” and “Sure”). It is possible that in this situation a judge model tries to evaluate one of the reference answers, thus marking it as correct, though further research is required to identify the cause of this behavior.', 'chunk_index': 64, 'metadata': {'languages': ['eng'], 'page_number': 8, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=328
Chunk is useful: {'title': 'Sure', 'page_content': '8 Sure Figure 5: We observe that judge models remain robust when exam-taker models produce responses identical to the prompt. However, this robustness diminishes in the presence of hallucinated responses such as "Yes" and "Sure" from the exam-taker models. Additionally, when assessing the gold standard answer, judges do not consistently arrive at the correct judgement 100% of the time. We use four different prompt versions, varying in length and specificity. Each prompt instructs the judge models on how to evaluate responses, with complexity and detail increasing with the token count. The first two prompts (Without guidelines V1 and V2, with 45 and 58 tokens, respectively) simply ask to evaluate the responses, without any further information, while more elaborate guidance and examples are given in the longer prompts (Guidelines without examples and Guidelines with examples, with 245 and 301 tokens, respectively). All prompts are listed in Appendix K. Figure 4b shows that Mistral 7B (Jiang et al., 2023), GPT-4 Turbo and Llama-3 70B exhibit relatively low variance in their agreement with humans as the level of information and the length of the prompt increases. For this task, top performers’ (GPT-4 Turbo and Llama-3 70B) implicit definition of a correct judgment seems well aligned with the provided instructions and thus shows high alignment with humans even if no specific instructions are provided. It can also be observed that only top performers appears to benefit from the more detailed instructions, with a slight upward trend, whereas the other models get less aligned with more instructions. This might be due to the less powerful judges not being able to follow many instructions in the prompt at the same time. Interestingly, in a follow-up experiment (see Appendix L), when the order of references provided to the judge models is shuffled, Figure 6b demonstrates that larger judge models consistently maintain their judgments regardless of the reference order, whereas smaller models except Mistral 7B are more sensitive to the reference order given in the prompt.', 'chunk_index': 64, 'metadata': {'languages': ['eng'], 'page_number': 8, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=270
Chunk is useful: {'title': 'Sure', 'page_content': '5.3 Evaluating controlled responses Next, we perform some simple tests on the judge models by asking them to evaluate a set of dummy benchmark responses. For the first test, the answer to be evaluated for each question is one of the references from the dataset, verbatinm (the answer is thus always correct), while for the next three tests, the answer to be evaluated is always incorrect, with the dummy exam-taker model always responding with “Yes”, and “Sure” for the second and the third tests, respectively, and simply repeating the question for the fourth test. Figure 5 shows that while some judge models are able to identify and correctly mark the answers as correct (for the first test) or incorrect (for the next three tests), some judges, notably Llama-2 70B incorrectly evaluates a significant number of dummy answers, even though it shows a relatively high alignment with humans on the benchmark evaluations (see Figure 1b). We hypothesise that when the answers are plausible but incorrect (e.g. if the question asks about the name of the author of a book, and the exam-taker model gives the name of the wrong author), most judges are able to identify them as being incorrect (by comparing it with the reference answer). However, the judges might get confused about what they are supposed to evaluate if the answer is completely unrelated to the question (such as the words “Yes” and “Sure”). It is possible that in this situation a judge model tries to evaluate one of the reference answers, thus marking it as correct, though further research is required to identify the cause of this behavior.', 'chunk_index': 64, 'metadata': {'languages': ['eng'], 'page_number': 8, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=1
Filtering out chunk, word_count=1 < threshold=10, {'title': 'Sure', 'page_content': '8', 'chunk_index': 64, 'metadata': {'languages': ['eng'], 'page_number': 8, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=40
Filtering out chunk, density_ratio=0.325 < threshold=0.5, {'title': 'Judge model', 'page_content': 'Judge model Gemma 2B Llama-2 7B Mistral 7B Llama-2 13B Llama-2 70B Llama-3 8B Llama-3 70B GPT-4 Turbo Judge model Judge model Gemma 2B Llama-2 7B Mistral 7B Llama-2 13B Llama-2 70B Llama-3 8B Llama-3 70B GPT-4 Turbo Judge model', 'chunk_index': 65, 'metadata': {'languages': ['eng'], 'page_number': 9, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=20
Filtering out chunk, density_ratio=0.45 < threshold=0.5, {'title': 'κ', 'page_content': 'κ 0.50 0.66 0.72 0.68 0.80 0.81 0.84 0.85 κ κ 0.50 0.66 0.72 0.68 0.80 0.81 0.84 0.85 κ', 'chunk_index': 66, 'metadata': {'languages': ['eng'], 'page_number': 9, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=20
Chunk is useful: {'title': 'Pc', 'page_content': 'Pc 0.62 0.68 0.75 0.76 0.79 0.80 0.84 0.85 P+ 0.80 0.36 0.75 0.45 0.94 0.84 0.79 0.66 102030405060708090100Consistency Score', 'chunk_index': 67, 'metadata': {'languages': ['eng'], 'page_number': 9, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=90
Chunk is useful: {'title': 'Pc', 'page_content': 'Pc P+ 0.19 Qwen 1.8B 0.19 Qwen 0.5B 0.40 Gemma 7B 0.50 Gemma 2B 0.69 Qwen 4B 0.66 Llama-2 7B Mistral 7B 0.72 Llama-2 13B 0.68 Llama-2 70B 0.80 0.80 Qwen 7B 0.81 Llama-3 8B Qwen 72B 0.82 0.83 Qwen 14B Llama-3 70B 0.84 0.85 GPT-4 Turbo 0.18 0.19 0.32 0.62 0.67 0.68 0.75 0.76 0.79 0.79 0.80 0.80 0.82 0.84 0.85 1.00 0.87 0.07 0.80 0.89 0.36 0.75 0.45 0.94 0.77 0.84 0.93 0.83 0.79 0.66 0.2 0.8 0.4 0.4 0.6 0.6 0.8 0.0 1.0Pc 0.2 0.0 (a) (b)', 'chunk_index': 67, 'metadata': {'languages': ['eng'], 'page_number': 9, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=24
Chunk is useful: {'title': 'Pc', 'page_content': 'Figure 17: a) Estimated values of Pc and P+ for different judge models. b) Pearson’s correlation coefficient between κ and Pc for judge models.', 'chunk_index': 67, 'metadata': {'languages': ['eng'], 'page_number': 9, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=21
Chunk is useful: {'title': 'Pc', 'page_content': '25 Pc 0.62 0.68 0.75 0.76 0.79 0.80 0.84 0.85 P+ 0.80 0.36 0.75 0.45 0.94 0.84 0.79 0.66 102030405060708090100Consistency Score', 'chunk_index': 67, 'metadata': {'languages': ['eng'], 'page_number': 9, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=90
Chunk is useful: {'title': 'Pc', 'page_content': 'Pc P+ 0.19 Qwen 1.8B 0.19 Qwen 0.5B 0.40 Gemma 7B 0.50 Gemma 2B 0.69 Qwen 4B 0.66 Llama-2 7B Mistral 7B 0.72 Llama-2 13B 0.68 Llama-2 70B 0.80 0.80 Qwen 7B 0.81 Llama-3 8B Qwen 72B 0.82 0.83 Qwen 14B Llama-3 70B 0.84 0.85 GPT-4 Turbo 0.18 0.19 0.32 0.62 0.67 0.68 0.75 0.76 0.79 0.79 0.80 0.80 0.82 0.84 0.85 1.00 0.87 0.07 0.80 0.89 0.36 0.75 0.45 0.94 0.77 0.84 0.93 0.83 0.79 0.66 0.2 0.8 0.4 0.4 0.6 0.6 0.8 0.0 1.0Pc 0.2 0.0 (a) (b)', 'chunk_index': 67, 'metadata': {'languages': ['eng'], 'page_number': 9, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=24
Chunk is useful: {'title': 'Pc', 'page_content': 'Figure 17: a) Estimated values of Pc and P+ for different judge models. b) Pearson’s correlation coefficient between κ and Pc for judge models.', 'chunk_index': 67, 'metadata': {'languages': ['eng'], 'page_number': 9, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=1
Filtering out chunk, word_count=1 < threshold=10, {'title': 'Pc', 'page_content': '25', 'chunk_index': 67, 'metadata': {'languages': ['eng'], 'page_number': 9, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=7
Filtering out chunk, word_count=7 < threshold=10, {'title': 'Llama-2 7BLlama-2 13BMistral 7BLlama-2 70BGPT-4', 'page_content': 'Llama-2 7BLlama-2 13BMistral 7BLlama-2 70BGPT-4 (a) (b)', 'chunk_index': 68, 'metadata': {'languages': ['eng'], 'page_number': 9, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=90
Chunk is useful: {'title': 'Llama-2 7BLlama-2 13BMistral 7BLlama-2 70BGPT-4', 'page_content': 'Figure 6: (a) Leniency bias across various judge models. To compute the leniency bias in judge models we estimate Pc and P+ for different judge models; (b) Consistency scores of each judge model for 3 random permutations of references in the prompt. The consistency score is the percentage of questions for which the judge model gives the same judgment for all 3 runs. We observe that the bigger models are more consistent as judges and less sensitive to reference orders in the prompt. More details are given in Appendix L.', 'chunk_index': 68, 'metadata': {'languages': ['eng'], 'page_number': 9, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=7
Filtering out chunk, word_count=7 < threshold=10, {'title': 'Llama-2 7BLlama-2 13BMistral 7BLlama-2 70BGPT-4', 'page_content': 'Llama-2 7BLlama-2 13BMistral 7BLlama-2 70BGPT-4 (a) (b)', 'chunk_index': 68, 'metadata': {'languages': ['eng'], 'page_number': 9, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=90
Chunk is useful: {'title': 'Llama-2 7BLlama-2 13BMistral 7BLlama-2 70BGPT-4', 'page_content': 'Figure 6: (a) Leniency bias across various judge models. To compute the leniency bias in judge models we estimate Pc and P+ for different judge models; (b) Consistency scores of each judge model for 3 random permutations of references in the prompt. The consistency score is the percentage of questions for which the judge model gives the same judgment for all 3 runs. We observe that the bigger models are more consistent as judges and less sensitive to reference orders in the prompt. More details are given in Appendix L.', 'chunk_index': 68, 'metadata': {'languages': ['eng'], 'page_number': 9, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=280
Chunk is useful: {'title': '5.4 Leniency bias in judge models', 'page_content': '5.4 Leniency bias in judge models To get a general sense of the inherent biases or misalignment in the evaluation criteria that might be present in the judge models, we estimate if they have a positive or negative bias in their judgment. To do so, we assume that a judge assigns the correct judgment (i.e. same evaluation as the ground truth) with a probability of Pc and assigns the rest of the samples to be “correct” with a probability P+, which we call their leniency bias. We estimate the values of Pc and P+ from the benchmark results,5 and show them in Figure 6a. We observe that P+ for most models is significantly higher than 0.5, indicating a tendency of the judge models to evaluate responses as “correct” when their evaluation criteria are not completely aligned with the provided instructions. 5.4 Leniency bias in judge models To get a general sense of the inherent biases or misalignment in the evaluation criteria that might be present in the judge models, we estimate if they have a positive or negative bias in their judgment. To do so, we assume that a judge assigns the correct judgment (i.e. same evaluation as the ground truth) with a probability of Pc and assigns the rest of the samples to be “correct” with a probability P+, which we call their leniency bias. We estimate the values of Pc and P+ from the benchmark results,5 and show them in Figure 6a. We observe that P+ for most models is significantly higher than 0.5, indicating a tendency of the judge models to evaluate responses as “correct” when their evaluation criteria are not completely aligned with the provided instructions.', 'chunk_index': 69, 'metadata': {'languages': ['eng'], 'page_number': 9, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=347
Chunk is useful: {'title': '6 Conclusion', 'page_content': '6 Conclusion In this work, we provide an extensive study of the properties of LLMs as judges, comparing them with human evaluation and automated evaluation methods. By focusing on a relatively ‘clean’ evaluation scenario in which inter-human agreement is high, we examine the potential issues with the LLM- as-a-judge paradigm separately from the ambiguity and subjectivity in the task itself. We find that even in relatively straightforward setups, smaller and more cost-efficient models are less effective judges compared to the best available LLMs – also if they are specifically trained as judges, such as JudgeLM – and that even the best LLMs fail to meet the consistency of humans. Furthermore, though previous work commonly used percent agreement, we found that Cohen’s kappa (κ) distinguishes judges much better. However, we observe that even judges with excellent κ scores may require further scrutiny. While GPT-4 Turbo and Llama-3 both have excellent alignment scores, simpler and more cost-efficient approaches like JudgeLM and contains perform better when discriminating between the exam-taker models in terms of their ranking, despite having much lower alignment scores and more systematic biases. In further analyses, we find that: 1) LLMs tend to judge positively when in doubt, and this is more pronounced for small models than for larger ones; 2) Judge models with lower alignment lack precision rather than recall; 3) GPT-4 Turbo and Llama-3 70B are generally robust across different prompts, but are difficult to ‘steer’ in their judgments; 4) Some judge models can be easily fooled by dummy answers such as ‘Yes’ and ‘Sure’; 5) Judge models are better at detecting completely incorrect answers than partially incorrect ones. Overall, this work adds to the realm of LLM evaluation research by assessing judges within a clearly defined and objective framework. Our results highlight the utility of using some LLMs as judges but also urge caution in blindly trusting their judgments, even if they are found to be well-aligned with 5The theoretical derivation of the expressions for Pc and P+, as well as the empirical validation for their estimated values can be found in Appendix M.', 'chunk_index': 70, 'metadata': {'languages': ['eng'], 'page_number': 9, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=94
Chunk is useful: {'title': '6 Conclusion', 'page_content': '9 humans. For practitioners using LLMs as judges – regardless of the setup – we recommend not only computing percent agreement, but also Cohen’s kappa, and pairing these with a qualitative analysis to ensure that conclusions from judge models are less susceptible to biases. We further elaborate on the limitations of our work in Appendix A. In the future, we plan to expand our work to increasingly more complex scenarios with more open-ended answers and variability, and more generally assess how consistent our findings are across dataset samples, benchmarks, and prompt templates. 6 Conclusion', 'chunk_index': 70, 'metadata': {'languages': ['eng'], 'page_number': 9, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=345
Chunk is useful: {'title': '6 Conclusion', 'page_content': 'In this work, we provide an extensive study of the properties of LLMs as judges, comparing them with human evaluation and automated evaluation methods. By focusing on a relatively ‘clean’ evaluation scenario in which inter-human agreement is high, we examine the potential issues with the LLM- as-a-judge paradigm separately from the ambiguity and subjectivity in the task itself. We find that even in relatively straightforward setups, smaller and more cost-efficient models are less effective judges compared to the best available LLMs – also if they are specifically trained as judges, such as JudgeLM – and that even the best LLMs fail to meet the consistency of humans. Furthermore, though previous work commonly used percent agreement, we found that Cohen’s kappa (κ) distinguishes judges much better. However, we observe that even judges with excellent κ scores may require further scrutiny. While GPT-4 Turbo and Llama-3 both have excellent alignment scores, simpler and more cost-efficient approaches like JudgeLM and contains perform better when discriminating between the exam-taker models in terms of their ranking, despite having much lower alignment scores and more systematic biases. In further analyses, we find that: 1) LLMs tend to judge positively when in doubt, and this is more pronounced for small models than for larger ones; 2) Judge models with lower alignment lack precision rather than recall; 3) GPT-4 Turbo and Llama-3 70B are generally robust across different prompts, but are difficult to ‘steer’ in their judgments; 4) Some judge models can be easily fooled by dummy answers such as ‘Yes’ and ‘Sure’; 5) Judge models are better at detecting completely incorrect answers than partially incorrect ones. Overall, this work adds to the realm of LLM evaluation research by assessing judges within a clearly defined and objective framework. Our results highlight the utility of using some LLMs as judges but also urge caution in blindly trusting their judgments, even if they are found to be well-aligned with 5The theoretical derivation of the expressions for Pc and P+, as well as the empirical validation for their estimated values can be found in Appendix M.', 'chunk_index': 70, 'metadata': {'languages': ['eng'], 'page_number': 9, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=92
Chunk is useful: {'title': '6 Conclusion', 'page_content': '9 humans. For practitioners using LLMs as judges – regardless of the setup – we recommend not only computing percent agreement, but also Cohen’s kappa, and pairing these with a qualitative analysis to ensure that conclusions from judge models are less susceptible to biases. We further elaborate on the limitations of our work in Appendix A. In the future, we plan to expand our work to increasingly more complex scenarios with more open-ended answers and variability, and more generally assess how consistent our findings are across dataset samples, benchmarks, and prompt templates.', 'chunk_index': 70, 'metadata': {'languages': ['eng'], 'page_number': 9, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=357
Chunk is useful: {'title': 'References', 'page_content': 'References Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609. Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. 2023. Open llm leaderboard. https: //huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard. Youssef Benchekroun, Megi Dervishi, Mark Ibrahim, Jean-Baptiste Gaya, Xavier Martinet, Grégoire Mialon, Thomas Scialom, Emmanuel Dupoux, Dieuwke Hupkes, and Pascal Vincent. 2023. Worldsense: A synthetic benchmark for grounded reasoning in large language models. arXiv preprint arXiv:2311.15930. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2023. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology. Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language models be an alternative to human Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. 2024. Chatbot arena: An open platform for evaluating LLMs by human preference.', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=46
Chunk is useful: {'title': 'References', 'page_content': 'Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. J. Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=311
Chunk is useful: {'title': 'References', 'page_content': 'evaluation of retrieval augmented generation. 10 Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. 2024. Gemma: Open models based on gemini research and technology. Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, and Sunayana Sitaram. 2023. Are large language model-based evaluators the solution to scaling up multilingual evaluation? arXiv preprint arXiv:2309.07462. Qianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He, Xunzhe Zhou, Jiaqing Liang, and Yanghua Xiao. 2024. Can large language models understand real-world complex instructions? Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):18188–18196.', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=21
Chunk is useful: {'title': 'References', 'page_content': 'Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding.', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=17
Chunk is useful: {'title': 'References', 'page_content': 'Xinyu Hu, Mingqi Gao, Sen Hu, Yang Zhang, Yicheng Chen, Teng Xu, and Xiaojun Wan. 2024. Are', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=25
Chunk is useful: {'title': 'References', 'page_content': 'LLM-based evaluators confusing nlg quality criteria? arXiv preprint arXiv:2402.12055. Hui Huang, Yingqi Qu, Jing Liu, Muyun Yang, and Tiejun Zhao. 2024. An empirical study of', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=67
Chunk is useful: {'title': 'References', 'page_content': 'LLM-as-a-Judge for LLM evaluation: Fine-tuned judge models are task-specific classifiers. Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. 2023. FinanceBench: A new benchmark for financial question answering. arXiv preprint arXiv:2311.11944. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7B. arXiv preprint arXiv:2310.06825.', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=14
Chunk is useful: {'title': 'References', 'page_content': 'TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551.', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=15
Chunk is useful: {'title': 'References', 'page_content': 'Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. 2023a. Generative', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=26
Chunk is useful: {'title': 'References', 'page_content': 'judge for evaluating alignment. arXiv preprint arXiv:2310.05470. Shiyang Li, Jun Yan, Hai Wang, Zheng Tang, Xiang Ren, Vijay Srinivasan, and Hongxia Jin. arXiv preprint 2023b. arXiv:2307.10558.', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=46
Chunk is useful: {'title': 'References', 'page_content': 'Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. TruthfulQA: Measuring how models mimic Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulic, Anna Korhonen, and Nigel Collier. 2024. Aligning with human judgement: The role of pairwise preference in large language model evaluators. arXiv preprint arXiv:2403.16950.', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=143
Chunk is useful: {'title': 'References', 'page_content': '11 Adian Liusie, Potsawee Manakul, and Mark Gales. 2024. LLM comparative assessment: Zero-shot NLG evaluation through pairwise comparisons using large language models. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 139–151, St. Julian’s, Malta. Association for Computational Linguistics. Lovish Madaan, Aaditya K. Singh, Rylan Schaeffer, Andrew Poulton, Sanmi Koyejo, Pontus Stenetorp, Sharan Narang, and Dieuwke Hupkes. 2024. Quantifying variance in evaluation benchmarks. arXiv preprint arXiv:/2406.10229. Oscar Mañas, Benno Krojer, and Aishwarya Agrawal. 2024. Improving automatic vqa evaluation using large language models. Proceedings of the AAAI Conference on Artificial Intelligence, 38(5):4171–4179. Xenia Ohmer, Elia Bruni, and Dieuwke Hupkes. 2024. From form (s) to meaning: Probing arXiv preprint the semantic depths of language models using multisense consistency. arXiv:2404.12145. Pouya Pezeshkpour and Estevam Hruschka. 2023. Large language models sensitivity to the order of', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=22
Chunk is useful: {'title': 'References', 'page_content': 'options in multiple-choice questions. arXiv preprint arXiv:2308.11483. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019.', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=25
Chunk is useful: {'title': 'References', 'page_content': 'Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Matthew Renze and Erhan Guven. 2024. The benefits of a concise chain of thought on problem-', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=22
Chunk is useful: {'title': 'References', 'page_content': 'solving in large language models. arXiv preprint arXiv:2401.05618. Keita Saito, Akifumi Wachi, Koki Wataoka, and Youhei Akimoto. 2023. Verbosity bias in preference', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=59
Chunk is useful: {'title': 'References', 'page_content': 'Shreya Shankar, JD Zamfirescu-Pereira, Björn Hartmann, Aditya G Parameswaran, and Ian Arawjo. 2024. Who validates the validators? aligning llm-assisted evaluation of llm outputs with human preferences. arXiv preprint arXiv:2404.12272. Andrea Sottana, Bin Liang, Kai Zou, and Zheng Yuan. 2023. Evaluation metrics in the era of gpt-4: reliably evaluating large language models on sequence to sequence tasks. arXiv preprint arXiv:2310.13800.', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=218
Chunk is useful: {'title': 'References', 'page_content': "C. Spearman. 1904. The proof and measurement of association between two things. The American Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Miles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. 2023. Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting. In Advances in Neural Information Processing Systems, volume 36, pages 74952–74965. Curran Associates, Inc. Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926. Minghao Wu and Alham Fikri Aji. 2023. Style over substance: Evaluation biases for large language Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, and Weiqiang Jia. 2023. Cognitive mirage: A review of hallucinations in large language models. arXiv preprint arXiv:2309.06794. Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. 2023. Evaluating large language models at evaluating instruction following. arXiv preprint arXiv:2310.07641. Yue Zhang, Ming Zhang, Haipeng Yuan, Shichun Liu, Yongyao Shi, Tao Gui, Qi Zhang, and Xuanjing Huang. 2024. Llmeval: A preliminary study on how to evaluate large language models. Proceedings of the AAAI Conference on Artificial Intelligence, 38(17):19615–19622.", 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=16
Chunk is useful: {'title': 'References', 'page_content': '12 Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. 2023. On large language', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=90
Chunk is useful: {'title': 'References', 'page_content': 'models’ selection bias in multi-choice questions. arXiv preprint arXiv:2309.03882. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. Advances in Neural Information Processing Systems, 36. Yuan Zhiqiang, Liu Junwei, Zi Qiancheng, Liu Mingwei, Peng Xin, Lou Yiling, et al. 2023. Evaluating instruction-tuned large language models on code comprehension and generation. arXiv e-prints arXiv:2308.01240. Lianghui Zhu, Xinggang Wang, and Xinlong Wang. 2023. Judgelm: Fine-tuned large language models are scalable judges.', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=324
Chunk is useful: {'title': 'References', 'page_content': '13 References References References References Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609. Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. 2023. Open llm leaderboard. https: //huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard. Youssef Benchekroun, Megi Dervishi, Mark Ibrahim, Jean-Baptiste Gaya, Xavier Martinet, Grégoire Mialon, Thomas Scialom, Emmanuel Dupoux, Dieuwke Hupkes, and Pascal Vincent. 2023. Worldsense: A synthetic benchmark for grounded reasoning in large language models. arXiv preprint arXiv:2311.15930. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2023. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology. Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language models be an alternative to human', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=70
Chunk is useful: {'title': 'References', 'page_content': 'Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. 2024. Chatbot arena: An open platform for evaluating LLMs by human preference. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=324
Chunk is useful: {'title': 'References', 'page_content': 'J. Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological evaluation of retrieval augmented generation. 10 Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. 2024. Gemma: Open models based on gemini research and technology. Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, and Sunayana Sitaram. 2023. Are large language model-based evaluators the solution to scaling up multilingual evaluation? arXiv preprint arXiv:2309.07462. Qianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He, Xunzhe Zhou, Jiaqing Liang, and Yanghua Xiao. 2024. Can large language models understand real-world complex instructions? Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):18188–18196.', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=21
Chunk is useful: {'title': 'References', 'page_content': 'Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding.', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=17
Chunk is useful: {'title': 'References', 'page_content': 'Xinyu Hu, Mingqi Gao, Sen Hu, Yang Zhang, Yicheng Chen, Teng Xu, and Xiaojun Wan. 2024. Are', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=25
Chunk is useful: {'title': 'References', 'page_content': 'LLM-based evaluators confusing nlg quality criteria? arXiv preprint arXiv:2402.12055. Hui Huang, Yingqi Qu, Jing Liu, Muyun Yang, and Tiejun Zhao. 2024. An empirical study of', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=67
Chunk is useful: {'title': 'References', 'page_content': 'LLM-as-a-Judge for LLM evaluation: Fine-tuned judge models are task-specific classifiers. Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. 2023. FinanceBench: A new benchmark for financial question answering. arXiv preprint arXiv:2311.11944. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7B. arXiv preprint arXiv:2310.06825.', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=14
Chunk is useful: {'title': 'References', 'page_content': 'TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551.', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=15
Chunk is useful: {'title': 'References', 'page_content': 'Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. 2023a. Generative', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=26
Chunk is useful: {'title': 'References', 'page_content': 'judge for evaluating alignment. arXiv preprint arXiv:2310.05470. Shiyang Li, Jun Yan, Hai Wang, Zheng Tang, Xiang Ren, Vijay Srinivasan, and Hongxia Jin. arXiv preprint 2023b. arXiv:2307.10558.', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=46
Chunk is useful: {'title': 'References', 'page_content': 'Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. TruthfulQA: Measuring how models mimic Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulic, Anna Korhonen, and Nigel Collier. 2024. Aligning with human judgement: The role of pairwise preference in large language model evaluators. arXiv preprint arXiv:2403.16950.', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=143
Chunk is useful: {'title': 'References', 'page_content': '11 Adian Liusie, Potsawee Manakul, and Mark Gales. 2024. LLM comparative assessment: Zero-shot NLG evaluation through pairwise comparisons using large language models. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 139–151, St. Julian’s, Malta. Association for Computational Linguistics. Lovish Madaan, Aaditya K. Singh, Rylan Schaeffer, Andrew Poulton, Sanmi Koyejo, Pontus Stenetorp, Sharan Narang, and Dieuwke Hupkes. 2024. Quantifying variance in evaluation benchmarks. arXiv preprint arXiv:/2406.10229. Oscar Mañas, Benno Krojer, and Aishwarya Agrawal. 2024. Improving automatic vqa evaluation using large language models. Proceedings of the AAAI Conference on Artificial Intelligence, 38(5):4171–4179. Xenia Ohmer, Elia Bruni, and Dieuwke Hupkes. 2024. From form (s) to meaning: Probing arXiv preprint the semantic depths of language models using multisense consistency. arXiv:2404.12145. Pouya Pezeshkpour and Estevam Hruschka. 2023. Large language models sensitivity to the order of', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=22
Chunk is useful: {'title': 'References', 'page_content': 'options in multiple-choice questions. arXiv preprint arXiv:2308.11483. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019.', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=25
Chunk is useful: {'title': 'References', 'page_content': 'Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Matthew Renze and Erhan Guven. 2024. The benefits of a concise chain of thought on problem-', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=81
Chunk is useful: {'title': 'References', 'page_content': 'solving in large language models. arXiv preprint arXiv:2401.05618. Keita Saito, Akifumi Wachi, Koki Wataoka, and Youhei Akimoto. 2023. Verbosity bias in preference Shreya Shankar, JD Zamfirescu-Pereira, Björn Hartmann, Aditya G Parameswaran, and Ian Arawjo. 2024. Who validates the validators? aligning llm-assisted evaluation of llm outputs with human preferences. arXiv preprint arXiv:2404.12272. Andrea Sottana, Bin Liang, Kai Zou, and Zheng Yuan. 2023. Evaluation metrics in the era of gpt-4: reliably evaluating large language models on sequence to sequence tasks. arXiv preprint arXiv:2310.13800.', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=218
Chunk is useful: {'title': 'References', 'page_content': "C. Spearman. 1904. The proof and measurement of association between two things. The American Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Miles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. 2023. Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting. In Advances in Neural Information Processing Systems, volume 36, pages 74952–74965. Curran Associates, Inc. Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926. Minghao Wu and Alham Fikri Aji. 2023. Style over substance: Evaluation biases for large language Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, and Weiqiang Jia. 2023. Cognitive mirage: A review of hallucinations in large language models. arXiv preprint arXiv:2309.06794. Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. 2023. Evaluating large language models at evaluating instruction following. arXiv preprint arXiv:2310.07641. Yue Zhang, Ming Zhang, Haipeng Yuan, Shichun Liu, Yongyao Shi, Tao Gui, Qi Zhang, and Xuanjing Huang. 2024. Llmeval: A preliminary study on how to evaluate large language models. Proceedings of the AAAI Conference on Artificial Intelligence, 38(17):19615–19622.", 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=16
Chunk is useful: {'title': 'References', 'page_content': '12 Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. 2023. On large language', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=90
Chunk is useful: {'title': 'References', 'page_content': 'models’ selection bias in multi-choice questions. arXiv preprint arXiv:2309.03882. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. Advances in Neural Information Processing Systems, 36. Yuan Zhiqiang, Liu Junwei, Zi Qiancheng, Liu Mingwei, Peng Xin, Lou Yiling, et al. 2023. Evaluating instruction-tuned large language models on code comprehension and generation. arXiv e-prints arXiv:2308.01240. Lianghui Zhu, Xinggang Wang, and Xinlong Wang. 2023. Judgelm: Fine-tuned large language models are scalable judges.', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=4
Filtering out chunk, word_count=4 < threshold=10, {'title': 'References', 'page_content': '13 References References References', 'chunk_index': 71, 'metadata': {'languages': ['eng'], 'page_number': 10, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=313
Chunk is useful: {'title': 'A Limitations', 'page_content': 'A Limitations In our work, we have shown potential pitfalls of using LLMs as judges, as well as the issues with commonly used metrics for quantifying their judgment quality. We have observed several trends and interesting phenomena regarding the behaviour of judge models in the LLM-as-a-judge paradigm, such as poor correlation or precision with alignment (§ 5.1), inability of smaller judge models to follow complex and nuanced instructions (§ 5.2), and tendencies of judge models to be lenient in their judgment when they are unsure (§ 5.4). Broadly, our results show that caution and scrutiny are required in using LLMs as judges, but not all our specific conclusions about rankings may hold across different examples, different judge models and exam-taker model accuracies, and tasks. For example, on a small experiment with a different sample and different instructions, we observed that when we limited the questions to only those that have fewer than 10 or 20 references, there was a consistent 5-10 point drop in percent-alignment across all judge models, but the discriminative ability of the top-performing judges improved. In that example, in which the overall accuracy of the models was substantially lower than in the example in our main experiment, we also observed a closer match between Cohen’s kappa and percent alignment, suggesting that the accuracies of the exam-taker models being compared may play an important role in how well their scores match. All in all, these differences underline how finicky using LLMs as judges can be, and with that confirm the overall conclusions of our study that much more work is needed to better understand the strengths and limitations of judge models across a wide range of scenarios and model accuracies. We consider assessing the strengths across multiple different samples and tasks, which would require many more human annotations, outside the scope of this paper and leave such experimentation for future work.', 'chunk_index': 72, 'metadata': {'languages': ['eng'], 'page_number': 14, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=40
Chunk is useful: {'title': 'A Limitations', 'page_content': 'B Model and dataset details In Table 3, we show the different models and datasets used in our experiments, along with version and license details. Table 3: Version and license details for the different models and datasets used in experiments.', 'chunk_index': 72, 'metadata': {'languages': ['eng'], 'page_number': 14, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=313
Chunk is useful: {'title': 'A Limitations', 'page_content': 'A Limitations In our work, we have shown potential pitfalls of using LLMs as judges, as well as the issues with commonly used metrics for quantifying their judgment quality. We have observed several trends and interesting phenomena regarding the behaviour of judge models in the LLM-as-a-judge paradigm, such as poor correlation or precision with alignment (§ 5.1), inability of smaller judge models to follow complex and nuanced instructions (§ 5.2), and tendencies of judge models to be lenient in their judgment when they are unsure (§ 5.4). Broadly, our results show that caution and scrutiny are required in using LLMs as judges, but not all our specific conclusions about rankings may hold across different examples, different judge models and exam-taker model accuracies, and tasks. For example, on a small experiment with a different sample and different instructions, we observed that when we limited the questions to only those that have fewer than 10 or 20 references, there was a consistent 5-10 point drop in percent-alignment across all judge models, but the discriminative ability of the top-performing judges improved. In that example, in which the overall accuracy of the models was substantially lower than in the example in our main experiment, we also observed a closer match between Cohen’s kappa and percent alignment, suggesting that the accuracies of the exam-taker models being compared may play an important role in how well their scores match. All in all, these differences underline how finicky using LLMs as judges can be, and with that confirm the overall conclusions of our study that much more work is needed to better understand the strengths and limitations of judge models across a wide range of scenarios and model accuracies. We consider assessing the strengths across multiple different samples and tasks, which would require many more human annotations, outside the scope of this paper and leave such experimentation for future work.', 'chunk_index': 72, 'metadata': {'languages': ['eng'], 'page_number': 14, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=40
Chunk is useful: {'title': 'A Limitations', 'page_content': 'B Model and dataset details In Table 3, we show the different models and datasets used in our experiments, along with version and license details. Table 3: Version and license details for the different models and datasets used in experiments.', 'chunk_index': 72, 'metadata': {'languages': ['eng'], 'page_number': 14, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=2
Filtering out chunk, word_count=2 < threshold=10, {'title': 'Asset', 'page_content': 'Asset Asset', 'chunk_index': 73, 'metadata': {'languages': ['eng'], 'page_number': 14, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=2
Filtering out chunk, word_count=2 < threshold=10, {'title': 'Version', 'page_content': 'Version Version', 'chunk_index': 74, 'metadata': {'languages': ['eng'], 'page_number': 14, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=92
Chunk is useful: {'title': 'License', 'page_content': 'License TriviaQA Llama-2 7B Base Llama-2 7B Chat Llama-2 13B Base Llama-2 13B Chat Llama-2 70B Base Llama-2 70B Chat Mistral 7B Base Mistral 7B Chat Llama-3 8B Chat Llama-3 70B Chat JudgeLM GPT-4 Turbo Qwen 0.5B Chat Qwen 1.8B Chat Qwen 4B Chat Qwen 7B Chat Qwen 14B Chat Qwen 72B Chat mandarjoshi/trivia_qa meta-llama/Llama-2-7b-hf meta-llama/Llama-2-7b-chat-hf meta-llama/Llama-2-13b-hf meta-llama/Llama-2-13b-chat-hf meta-llama/Llama-2-70b-hf meta-llama/Llama-2-70b-chat-hf mistralai/Mistral-7B-v0.1 mistralai/Mistral-7B-Instruct-v0.2 meta-llama/Meta-Llama-3-8B-Instruct meta-llama/Meta-Llama-3-70B-Instruct BAAI/JudgeLM-7B-v1.0 gpt-4-turbo-2024-04-09 Qwen/Qwen1.5-0.5B-Chat Qwen/Qwen1.5-1.8B-Chat Qwen/Qwen1.5-4B-Chat Qwen/Qwen1.5-7B-Chat Qwen/Qwen1.5-14B-Chat Qwen/Qwen1.5-72B-Chat apache-2.0 llama2 llama2 llama2 llama2 llama2 llama2 apache-2.0 apache-2.0 llama3 llama3 Non-commercial license N/A tongyi-qianwen tongyi-qianwen tongyi-qianwen tongyi-qianwen tongyi-qianwen tongyi-qianwen', 'chunk_index': 75, 'metadata': {'languages': ['eng'], 'page_number': 14, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=92
Chunk is useful: {'title': 'License', 'page_content': 'License TriviaQA Llama-2 7B Base Llama-2 7B Chat Llama-2 13B Base Llama-2 13B Chat Llama-2 70B Base Llama-2 70B Chat Mistral 7B Base Mistral 7B Chat Llama-3 8B Chat Llama-3 70B Chat JudgeLM GPT-4 Turbo Qwen 0.5B Chat Qwen 1.8B Chat Qwen 4B Chat Qwen 7B Chat Qwen 14B Chat Qwen 72B Chat mandarjoshi/trivia_qa meta-llama/Llama-2-7b-hf meta-llama/Llama-2-7b-chat-hf meta-llama/Llama-2-13b-hf meta-llama/Llama-2-13b-chat-hf meta-llama/Llama-2-70b-hf meta-llama/Llama-2-70b-chat-hf mistralai/Mistral-7B-v0.1 mistralai/Mistral-7B-Instruct-v0.2 meta-llama/Meta-Llama-3-8B-Instruct meta-llama/Meta-Llama-3-70B-Instruct BAAI/JudgeLM-7B-v1.0 gpt-4-turbo-2024-04-09 Qwen/Qwen1.5-0.5B-Chat Qwen/Qwen1.5-1.8B-Chat Qwen/Qwen1.5-4B-Chat Qwen/Qwen1.5-7B-Chat Qwen/Qwen1.5-14B-Chat Qwen/Qwen1.5-72B-Chat apache-2.0 llama2 llama2 llama2 llama2 llama2 llama2 apache-2.0 apache-2.0 llama3 llama3 Non-commercial license N/A tongyi-qianwen tongyi-qianwen tongyi-qianwen tongyi-qianwen tongyi-qianwen tongyi-qianwen', 'chunk_index': 75, 'metadata': {'languages': ['eng'], 'page_number': 14, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=29
Chunk is useful: {'title': 'C Model evaluation prompt templates', 'page_content': 'C Model evaluation prompt templates In Figure 7 and Figure 8, we show the prompt templates used for the base and chat exam-taker models during the question answering process.', 'chunk_index': 76, 'metadata': {'languages': ['eng'], 'page_number': 14, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=153
Chunk is useful: {'title': 'C Model evaluation prompt templates', 'page_content': '14 Prompt template for B models exam: Q: Can you name the actress who links ’The Darling Buds of May’ and *Rosemary and Thyme’? A: Pam Ferris Q: A neologism is a new? A: Word/expression Q: Who, in 2010, became the first person from outside the British Isles to win the World Snooker Championship title since Cliff Thorburn in 1980, and the first non British player to win the title since Ken Doherty in 19977 A: Neil Robertson Q: Which German Nazi leader flew solo from Ausberg in 1941 and landed by parachute near Glasgow on a private peace mission? A: Hess Q: Where would you find Narita airport? A: Tokyo, Japan : Which cartoon title character has a friend called Captain Haddock? Po C Model evaluation prompt templates In Figure 7 and Figure 8, we show the prompt templates used for the base and chat exam-taker models during the question answering process.', 'chunk_index': 76, 'metadata': {'languages': ['eng'], 'page_number': 14, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=124
Chunk is useful: {'title': 'C Model evaluation prompt templates', 'page_content': '14 Prompt template for B models exam: Q: Can you name the actress who links ’The Darling Buds of May’ and *Rosemary and Thyme’? A: Pam Ferris Q: A neologism is a new? A: Word/expression Q: Who, in 2010, became the first person from outside the British Isles to win the World Snooker Championship title since Cliff Thorburn in 1980, and the first non British player to win the title since Ken Doherty in 19977 A: Neil Robertson Q: Which German Nazi leader flew solo from Ausberg in 1941 and landed by parachute near Glasgow on a private peace mission? A: Hess Q: Where would you find Narita airport? A: Tokyo, Japan : Which cartoon title character has a friend called Captain Haddock? Po', 'chunk_index': 76, 'metadata': {'languages': ['eng'], 'page_number': 14, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=342
Chunk is useful: {'title': 'Figure 7: Prompt template for base exam-taker models', 'page_content': 'Figure 7: Prompt template for base exam-taker models Prompt template for Chat exam-taker models You are a part of a question answering benchmark. Look at the following examples on how to answer the questions Q: Can you name the actress who links ’The Darling Buds of May’ and *Rosemary and Thyme’? A: Pam Ferris Q: A neologism is a new? A: Word/expression Q: Who, in 2010, became the first person from outside the British Isles to win the World Snooker Championship title since Cliff Thorburn in 1980, and the first non British player to win the title since Ken Doherty in 19977 A: Neil Robertson Q: Which German Nazi leader flew solo from Ausberg in 1941 and landed by parachute near Glasgow on a private peace mission? A: Hess Q: Where would you find Narita airport? A: Tokyo, Japan Your task is to answer the following question. Remember to be concise and only give the answer in a few words Q:Which cartoon title character has a friend called Captain Haddock? A Figure 7: Prompt template for base exam-taker models Prompt template for Chat exam-taker models You are a part of a question answering benchmark. Look at the following examples on how to answer the questions Q: Can you name the actress who links ’The Darling Buds of May’ and *Rosemary and Thyme’? A: Pam Ferris Q: A neologism is a new? A: Word/expression Q: Who, in 2010, became the first person from outside the British Isles to win the World Snooker Championship title since Cliff Thorburn in 1980, and the first non British player to win the title since Ken Doherty in 19977 A: Neil Robertson Q: Which German Nazi leader flew solo from Ausberg in 1941 and landed by parachute near Glasgow on a private peace mission? A: Hess Q: Where would you find Narita airport? A: Tokyo, Japan Your task is to answer the following question. Remember to be concise and only give the answer in a few words Q:Which cartoon title character has a friend called Captain Haddock? A', 'chunk_index': 77, 'metadata': {'languages': ['eng'], 'page_number': 15, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=18
Chunk is useful: {'title': 'Figure 8: Prompt template for Chat exam-taker models', 'page_content': 'Figure 8: Prompt template for Chat exam-taker models 15 Figure 8: Prompt template for Chat exam-taker models 15', 'chunk_index': 78, 'metadata': {'languages': ['eng'], 'page_number': 15, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=108
Chunk is useful: {'title': 'D Judge LLM Prompt templates', 'page_content': 'D Judge LLM Prompt templates In Figure 9, we show the prompt template used to guide the judge models during the evaluation process of a 400-question sample from the TriviaQA unfiltered dataset. Prompt template for judge models Your task is to look at the following question, and based on the references provided, determine if the model’s response is correct or incorrect. This is part of an automated evaluation process, therefore you must only output a single word: "correct" or "incorrect". Question: Which Australian did Roger Federer defeat to win his first Wimbledon Men’s Singles title in 20037 References: MARK PHILIPPOUSSIS MARK PHILIPPOUSSIS Model Response: Mark Philippoussis Evaluation (correct/incorrect):', 'chunk_index': 79, 'metadata': {'languages': ['eng'], 'page_number': 16, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=32
Chunk is useful: {'title': 'D Judge LLM Prompt templates', 'page_content': 'D Judge LLM Prompt templates In Figure 9, we show the prompt template used to guide the judge models during the evaluation process of a 400-question sample from the TriviaQA unfiltered dataset.', 'chunk_index': 79, 'metadata': {'languages': ['eng'], 'page_number': 16, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=76
Chunk is useful: {'title': 'D Judge LLM Prompt templates', 'page_content': 'Prompt template for judge models Your task is to look at the following question, and based on the references provided, determine if the model’s response is correct or incorrect. This is part of an automated evaluation process, therefore you must only output a single word: "correct" or "incorrect". Question: Which Australian did Roger Federer defeat to win his first Wimbledon Men’s Singles title in 20037 References: MARK PHILIPPOUSSIS MARK PHILIPPOUSSIS Model Response: Mark Philippoussis Evaluation (correct/incorrect):', 'chunk_index': 79, 'metadata': {'languages': ['eng'], 'page_number': 16, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=16
Chunk is useful: {'title': 'Figure 9: Prompt templates for the judge models', 'page_content': 'Figure 9: Prompt templates for the judge models Figure 9: Prompt templates for the judge models', 'chunk_index': 80, 'metadata': {'languages': ['eng'], 'page_number': 16, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=103
Chunk is useful: {'title': 'E Metrics for judge models', 'page_content': 'E Metrics for judge models If one of the annotators is taken to be the reference, then the annotations of the other annotator can be categorized as true positives, false positives, true negatives, and false negatives, with the total number of each of them in a benchmark being represented by TP , FP , TN , and FN respectively. Percent agreement is simply the ratio of the numbers of times two annotators agree with each other relative to the total number of annotations. This ratio can have values between 0 and 1. For the binary case, the alignment ratio ρ is given as', 'chunk_index': 81, 'metadata': {'languages': ['eng'], 'page_number': 16, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=55
Chunk is useful: {'title': 'E Metrics for judge models', 'page_content': '. Cohen’s kappa coefficient, or Cohen’s kappa for short (Cohen, 1960), measures the alignment of two annotators while also taking into account the possibility of agreement by pure chance. This coefficient can have values between −1 and 1, but is usually above 0 in most real-world situations. The value of Cohen’s kappa is given as', 'chunk_index': 81, 'metadata': {'languages': ['eng'], 'page_number': 16, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=28
Filtering out chunk, density_ratio=0.4642857142857143 < threshold=0.5, {'title': 'E Metrics for judge models', 'page_content': 'po − pe 1 − pe = 2(TP TN − FP FN ) (TP + FP )(TN + FP ) + (TP + FN )(TN + FN )', 'chunk_index': 81, 'metadata': {'languages': ['eng'], 'page_number': 16, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=187
Chunk is useful: {'title': 'E Metrics for judge models', 'page_content': '. Here, po is the relative observed agreement, and pe is the hypothetical probability of chance agreement. This coefficient is considered to be a more robust measure of inter-annotator alignment, but also less interpretable in terms of what a particular value of κ means. Generally, values of κ in ranges [0, 0.2), [0.2, 0.4), [0.4, 0.6), [0.6, 0.8), and [0.8, 1) are considered to indicate no alignment, slight alignment, moderate alignment, substantial alignment, and near-perfect alignment respectively, with κ = 1 indicating perfect alignment. E Metrics for judge models If one of the annotators is taken to be the reference, then the annotations of the other annotator can be categorized as true positives, false positives, true negatives, and false negatives, with the total number of each of them in a benchmark being represented by TP , FP , TN , and FN respectively. Percent agreement is simply the ratio of the numbers of times two annotators agree with each other relative to the total number of annotations. This ratio can have values between 0 and 1. For the binary case, the alignment ratio ρ is given as', 'chunk_index': 81, 'metadata': {'languages': ['eng'], 'page_number': 16, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=83
Chunk is useful: {'title': 'E Metrics for judge models', 'page_content': '. Cohen’s kappa coefficient, or Cohen’s kappa for short (Cohen, 1960), measures the alignment of two annotators while also taking into account the possibility of agreement by pure chance. This coefficient can have values between −1 and 1, but is usually above 0 in most real-world situations. The value of Cohen’s kappa is given as po − pe 1 − pe = 2(TP TN − FP FN ) (TP + FP )(TN + FP ) + (TP + FN )(TN + FN )', 'chunk_index': 81, 'metadata': {'languages': ['eng'], 'page_number': 16, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=84
Chunk is useful: {'title': 'E Metrics for judge models', 'page_content': '. Here, po is the relative observed agreement, and pe is the hypothetical probability of chance agreement. This coefficient is considered to be a more robust measure of inter-annotator alignment, but also less interpretable in terms of what a particular value of κ means. Generally, values of κ in ranges [0, 0.2), [0.2, 0.4), [0.4, 0.6), [0.6, 0.8), and [0.8, 1) are considered to indicate no alignment, slight alignment, moderate alignment, substantial alignment, and near-perfect alignment respectively, with κ = 1 indicating perfect alignment.', 'chunk_index': 81, 'metadata': {'languages': ['eng'], 'page_number': 16, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=32
Chunk is useful: {'title': 'F Human Annotation Guidelines', 'page_content': 'F Human Annotation Guidelines We provide the guidelines used for human evaluation below. 16 (1) (2) F Human Annotation Guidelines We provide the guidelines used for human evaluation below. 16 (1) (2)', 'chunk_index': 82, 'metadata': {'languages': ['eng'], 'page_number': 16, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=67
Chunk is useful: {'title': 'Humn annotation guidelines', 'page_content': 'Humn annotation guidelines You will be given a question, a set of reference answers and the answer given by an LLM. Your task is to judge if the answer given by the LLM is correct, as if you were the LLMs teacher grading their exam. equivalent to (one of the) reference answers. follow the following guidelines: An answer should be counted as correct if it is semantically', 'chunk_index': 83, 'metadata': {'languages': ['eng'], 'page_number': 17, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=124
Chunk is useful: {'title': 'Humn annotation guidelines', 'page_content': 'In doing so, please Underspecified answers (e.g. 20") should be marked incorrect. Underspecified answers (e.g. 20") should be marked incorrect. Answers that have more information than requested (e.g. "December 20, in Paris" instead of "December 20") should be marked correct, provided the extra information is not incorrect or contrasting the rest of the answer. Answers with unnecessary verbosity but correct answers should “Thanks for asking this question! The Answers with unnecessary verbosity but correct answers should “Thanks for asking this question! The If you have trouble judging whether the answer is correct, for instance because you feel you are lacking knowledge required to judge so, please indicate so by marking the answer "maybe correct" or “maybe incorrect", so that we can further review it.', 'chunk_index': 83, 'metadata': {'languages': ['eng'], 'page_number': 17, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=67
Chunk is useful: {'title': 'Humn annotation guidelines', 'page_content': 'Humn annotation guidelines You will be given a question, a set of reference answers and the answer given by an LLM. Your task is to judge if the answer given by the LLM is correct, as if you were the LLMs teacher grading their exam. equivalent to (one of the) reference answers. follow the following guidelines: An answer should be counted as correct if it is semantically', 'chunk_index': 83, 'metadata': {'languages': ['eng'], 'page_number': 17, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=124
Chunk is useful: {'title': 'Humn annotation guidelines', 'page_content': 'In doing so, please Underspecified answers (e.g. 20") should be marked incorrect. Underspecified answers (e.g. 20") should be marked incorrect. Answers that have more information than requested (e.g. "December 20, in Paris" instead of "December 20") should be marked correct, provided the extra information is not incorrect or contrasting the rest of the answer. Answers with unnecessary verbosity but correct answers should “Thanks for asking this question! The Answers with unnecessary verbosity but correct answers should “Thanks for asking this question! The If you have trouble judging whether the answer is correct, for instance because you feel you are lacking knowledge required to judge so, please indicate so by marking the answer "maybe correct" or “maybe incorrect", so that we can further review it.', 'chunk_index': 83, 'metadata': {'languages': ['eng'], 'page_number': 17, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=137
Chunk is useful: {'title': 'G Experiment costs', 'page_content': 'G Experiment costs The costs for the different experiments described in this work belong in three categories – GPU-hours for running open-source models on one or more Nvidia A100 GPUs, OpenAI credits for making API calls to OpenAI models,6 and human hours for manual annotations of benchmark responses. The estimated costs for the final reported experiments are given in Table 4. In addition to this, previous unreported experiments and trials had an approximate cost of 120 GPU-hours, 100 USD in OpenAI credits, and 50 human hours, bringing the total experimental cost for this work to approximately 200 GPU-hours, USD 125 OpenAI credits, and 75 human annotation hours. Table 4: Estimated costs for the final reported experiments. GPU-hours are in equivalent Nvidia A100 hours, OpenAI credits are in USD, and human hours are time spent in manual annotation.', 'chunk_index': 84, 'metadata': {'languages': ['eng'], 'page_number': 17, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=137
Chunk is useful: {'title': 'G Experiment costs', 'page_content': 'G Experiment costs The costs for the different experiments described in this work belong in three categories – GPU-hours for running open-source models on one or more Nvidia A100 GPUs, OpenAI credits for making API calls to OpenAI models,6 and human hours for manual annotations of benchmark responses. The estimated costs for the final reported experiments are given in Table 4. In addition to this, previous unreported experiments and trials had an approximate cost of 120 GPU-hours, 100 USD in OpenAI credits, and 50 human hours, bringing the total experimental cost for this work to approximately 200 GPU-hours, USD 125 OpenAI credits, and 75 human annotation hours. Table 4: Estimated costs for the final reported experiments. GPU-hours are in equivalent Nvidia A100 hours, OpenAI credits are in USD, and human hours are time spent in manual annotation.', 'chunk_index': 84, 'metadata': {'languages': ['eng'], 'page_number': 17, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=46
Chunk is useful: {'title': 'Experiment', 'page_content': 'Experiment GPU-hours OpenAI credits Human hours Main benchmarks Main evaluations Human alignment Error analysis Controlled responses Leniency bias Guideline bias Reference bias 5 30 1 1.5 15 5 10 5 2 8 - - - 5 5 4 10 6 5 - - 1 1 23', 'chunk_index': 85, 'metadata': {'languages': ['eng'], 'page_number': 17, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=46
Chunk is useful: {'title': 'Experiment', 'page_content': 'Experiment GPU-hours OpenAI credits Human hours Main benchmarks Main evaluations Human alignment Error analysis Controlled responses Leniency bias Guideline bias Reference bias 5 30 1 1.5 15 5 10 5 2 8 - - - 5 5 4 10 6 5 - - 1 1 23', 'chunk_index': 85, 'metadata': {'languages': ['eng'], 'page_number': 17, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=6
Filtering out chunk, word_count=6 < threshold=10, {'title': 'Total', 'page_content': 'Total 72.5 24 Total 72.5 24', 'chunk_index': 86, 'metadata': {'languages': ['eng'], 'page_number': 17, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=32
Chunk is useful: {'title': 'H Judge Scores', 'page_content': 'H Judge Scores We show the scores assigned by each judge model to each exam-taker model, visualised in Figure 1a in Table 5. 6Pricing details for OpenAI models are available at https://openai.com/api/pricing/', 'chunk_index': 87, 'metadata': {'languages': ['eng'], 'page_number': 17, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=33
Chunk is useful: {'title': 'H Judge Scores', 'page_content': '17 H Judge Scores We show the scores assigned by each judge model to each exam-taker model, visualised in Figure 1a in Table 5. 6Pricing details for OpenAI models are available at https://openai.com/api/pricing/', 'chunk_index': 87, 'metadata': {'languages': ['eng'], 'page_number': 17, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=1
Filtering out chunk, word_count=1 < threshold=10, {'title': 'H Judge Scores', 'page_content': '17', 'chunk_index': 87, 'metadata': {'languages': ['eng'], 'page_number': 17, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=6
Filtering out chunk, word_count=6 < threshold=10, {'title': 'Exam taker models', 'page_content': 'Exam taker models Exam taker models', 'chunk_index': 88, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=2
Filtering out chunk, word_count=2 < threshold=10, {'title': 'Llama2', 'page_content': 'Llama2 Llama2', 'chunk_index': 89, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=4
Filtering out chunk, word_count=4 < threshold=10, {'title': 'Judge Models', 'page_content': 'Judge Models Judge Models', 'chunk_index': 90, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=6
Filtering out chunk, word_count=6 < threshold=10, {'title': '7B', 'page_content': '7B 7B 7B 7B 7B 7B', 'chunk_index': 91, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=6
Filtering out chunk, word_count=6 < threshold=10, {'title': 'Base 13B', 'page_content': 'Base 13B 70B Base 13B 70B', 'chunk_index': 92, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=6
Filtering out chunk, word_count=6 < threshold=10, {'title': 'Chat 13B', 'page_content': 'Chat 13B 70B Chat 13B 70B', 'chunk_index': 93, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=6
Filtering out chunk, word_count=6 < threshold=10, {'title': 'Base', 'page_content': 'Base Base 108771616732211 Base Base 108771616732211', 'chunk_index': 94, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=2
Filtering out chunk, word_count=2 < threshold=10, {'title': 'Instruct', 'page_content': 'Instruct Instruct', 'chunk_index': 95, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=24
Chunk is useful: {'title': 'Llama 3 8B Llama 3 70B', 'page_content': 'Llama 3 8B Llama 3 70B 68.75 76.00 66.00 76.50 85.25 87.00 52.91 61.52 75.0 68.50 78.5 78.25 62.5 73.5 66.00 65.00 76.25 94.50', 'chunk_index': 96, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=24
Chunk is useful: {'title': 'Llama 3 8B Llama 3 70B', 'page_content': 'Llama 3 8B Llama 3 70B 68.75 76.00 66.00 76.50 85.25 87.00 52.91 61.52 75.0 68.50 78.5 78.25 62.5 73.5 66.00 65.00 76.25 94.50', 'chunk_index': 96, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=18
Chunk is useful: {'title': 'Llama 2 7B Llama 2 13B Llama 2 70B', 'page_content': 'Llama 2 7B Llama 2 13B Llama 2 70B 61.75 53.00 68.25 71.25 80.25 61.50 66.50 76.30 86.50', 'chunk_index': 97, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=18
Chunk is useful: {'title': 'Llama 2 7B Llama 2 13B Llama 2 70B', 'page_content': '58.48 46.07 64.05 77.25 27.75 80.04 58.00 39.50 82.25 68.25 57.50 77.00 80.50 39.00 74.25 86.75 73.50 95.25', 'chunk_index': 97, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=21
Chunk is useful: {'title': 'Llama 2 7B Llama 2 13B Llama 2 70B', 'page_content': 'Llama 2 7B Llama 2 13B Llama 2 70B 61.75 53.00 68.25 71.25 80.25 61.50 66.50 76.30 86.50 58.48 46.07 64.05', 'chunk_index': 97, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=15
Chunk is useful: {'title': 'Llama 2 7B Llama 2 13B Llama 2 70B', 'page_content': '77.25 27.75 80.04 58.00 39.50 82.25 68.25 57.50 77.00 80.50 39.00 74.25 86.75 73.50 95.25', 'chunk_index': 97, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=10
Chunk is useful: {'title': 'JudgeLM', 'page_content': 'JudgeLM 52.50 60.50 64.00 41.52 32.50 60.00 57.25 45.5 68.50', 'chunk_index': 98, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=10
Chunk is useful: {'title': 'JudgeLM', 'page_content': 'JudgeLM 52.50 60.50 64.00 41.52 32.50 60.00 57.25 45.5 68.50', 'chunk_index': 98, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=22
Chunk is useful: {'title': 'Exact Match Contains Match', 'page_content': 'Exact Match Contains Match 46.75 50.75 60.00 56.00 63.75 68.00 24.05 38.98 0.25 46.25 36.25 59.50 59.50 57.25 20.25 44.00 58.25 70.00', 'chunk_index': 99, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=22
Chunk is useful: {'title': 'Exact Match Contains Match', 'page_content': 'Exact Match Contains Match 46.75 50.75 60.00 56.00 63.75 68.00 24.05 38.98 0.25 46.25 36.25 59.50 59.50 57.25 20.25 44.00 58.25 70.00', 'chunk_index': 99, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=11
Chunk is useful: {'title': 'Human Eval', 'page_content': 'Human Eval 62.25 72.75 83.75 56.00 56.50 72.25 71.75 60.75 91.50', 'chunk_index': 100, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=11
Chunk is useful: {'title': 'Human Eval', 'page_content': 'Human Eval 62.25 72.75 83.75 56.00 56.50 72.25 71.75 60.75 91.50', 'chunk_index': 100, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=185
Chunk is useful: {'title': 'I Exam-taker model base vs chat analysis', 'page_content': 'I Exam-taker model base vs chat analysis Given the human judgments we have available, we take the opportunity to investigate the performance differences between base and their corresponding chat models. In Table 6, we show the scores assigned by various judge models to four base-chat pairs. According to the default metric EM, the base models outperform the chat models by a large margin. Interestingly, while this difference gets smaller when the answers are judged by humans (second column) or GPT-4 Turbo, there is still a substantial difference for all four pairs, suggesting that the difference is not merely an effect of the increased verbosity of the chat models. Further evidence for that hypothesis is provided by Figure 10b, in which we can see that while 14% of the errors are shared between the base-chat pairs, almost another 14% of the examples get judged correctly by the base models but not by the chat models, while the opposite happens in only 2.5% of the cases. We consider two alternative hypotheses: i) The chat models have a worse understanding of the particular prompt format, which is tuned', 'chunk_index': 101, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=287
Chunk is useful: {'title': 'I Exam-taker model base vs chat analysis', 'page_content': 'more to fit base models; or ii) The chat models have ‘unlearned’ some knowledge during their alignment training. To disentangle these two factors, we manually analyse 400 questions for Llama-2 70B and Llama-2 70B-chat, using our earlier error codes. The results, shown in Figure 10a, sugest that, at least to some extent, the difference between base and chat models is in fact due to ‘unlearning’ of knowledge: while the number of errors is more or less equal among most categories, there is a stark difference in the incorrect entity category. Substantially more often than the base models, the chat models do answer I Exam-taker model base vs chat analysis Given the human judgments we have available, we take the opportunity to investigate the performance differences between base and their corresponding chat models. In Table 6, we show the scores assigned by various judge models to four base-chat pairs. According to the default metric EM, the base models outperform the chat models by a large margin. Interestingly, while this difference gets smaller when the answers are judged by humans (second column) or GPT-4 Turbo, there is still a substantial difference for all four pairs, suggesting that the difference is not merely an effect of the increased verbosity of the chat models. Further evidence for that hypothesis is provided by Figure 10b, in which we can see that while 14% of the errors are shared between the base-chat pairs, almost another 14% of the examples get judged correctly by the base models but not by the chat models, while the opposite happens in only 2.5% of the cases. We consider two alternative hypotheses: i) The chat models have a worse understanding of the particular prompt format, which is tuned', 'chunk_index': 101, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=102
Chunk is useful: {'title': 'I Exam-taker model base vs chat analysis', 'page_content': 'more to fit base models; or ii) The chat models have ‘unlearned’ some knowledge during their alignment training. To disentangle these two factors, we manually analyse 400 questions for Llama-2 70B and Llama-2 70B-chat, using our earlier error codes. The results, shown in Figure 10a, sugest that, at least to some extent, the difference between base and chat models is in fact due to ‘unlearning’ of knowledge: while the number of errors is more or less equal among most categories, there is a stark difference in the incorrect entity category. Substantially more often than the base models, the chat models do answer', 'chunk_index': 101, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=22
Chunk is useful: {'title': 'Table 6: Scores of base and chat models by various judges', 'page_content': 'Table 6: Scores of base and chat models by various judges Table 6: Scores of base and chat models by various judges', 'chunk_index': 102, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=4
Filtering out chunk, word_count=4 < threshold=10, {'title': 'Judge models', 'page_content': 'Judge models Judge models', 'chunk_index': 103, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=4
Filtering out chunk, word_count=4 < threshold=10, {'title': 'Base-Chat pair', 'page_content': 'Base-Chat pair Base-Chat pair', 'chunk_index': 104, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=4
Filtering out chunk, word_count=4 < threshold=10, {'title': 'GPT-4 Turbo', 'page_content': 'GPT-4 Turbo GPT-4 Turbo', 'chunk_index': 105, 'metadata': {'languages': ['eng'], 'page_number': 18, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=4
Filtering out chunk, word_count=4 < threshold=10, {'title': 'IncorrectentityUnder-specifiedToo fewentitiesOtherToomanyNoanswer', 'page_content': 'IncorrectentityUnder-specifiedToo fewentitiesOtherToomanyNoanswer IncorrectentityUnder-specifiedToo fewentitiesOtherToomanyNoanswer', 'chunk_index': 106, 'metadata': {'languages': ['eng'], 'page_number': 19, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=2
Filtering out chunk, word_count=2 < threshold=10, {'title': 'Chat', 'page_content': 'Chat Chat', 'chunk_index': 107, 'metadata': {'languages': ['eng'], 'page_number': 19, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=238
Chunk is useful: {'title': '020406080100120No of Questions Wrong', 'page_content': '020406080100120No of Questions Wrong Both correct Both incorrect Base correct, Chat incorrect 13.75%14.00%69.75%2.50% Base incorrect, Chat correct (a) (b) Figure 10: a) Distribution of incorrect question counts by error codes for the Llama2 70B Base vs Chat exam-taker models evaluated on 400 questions. b) Pie chart showing the percentage of questions categorized by the judgment from Base and Chat models. the question with a semantically plausible but incorrect entity. In Table 7-9, we provide examples of such cases. The results do not show any evidence to support the first hypothesis: the number of errors where the answer cannot be parsed or is just entirely incorrect does not differ between base and chat models. Table 7: Knowledge unlearning example 1. 020406080100120No of Questions Wrong Both correct Both incorrect Base correct, Chat incorrect 13.75%14.00%69.75%2.50% Base incorrect, Chat correct (a) (b) Figure 10: a) Distribution of incorrect question counts by error codes for the Llama2 70B Base vs Chat exam-taker models evaluated on 400 questions. b) Pie chart showing the percentage of questions categorized by the judgment from Base and Chat models. the question with a semantically plausible but incorrect entity. In Table 7-9, we provide examples of such cases. The results do not show any evidence to support the first hypothesis: the number of errors where the answer cannot be parsed or is just entirely incorrect does not differ between base and chat models. Table 7: Knowledge unlearning example 1.', 'chunk_index': 108, 'metadata': {'languages': ['eng'], 'page_number': 19, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=36
Filtering out chunk, density_ratio=0.16666666666666666 < threshold=0.5, {'title': 'LLama-2 70B Base LLama-2 70B Chat Mistral 7B Base', 'page_content': 'LLama-2 70B Base LLama-2 70B Chat Mistral 7B Base LLama-2 70B Base LLama-2 70B Chat Mistral 7B Base LLama-2 70B Base LLama-2 70B Chat Mistral 7B Base LLama-2 70B Base LLama-2 70B Chat Mistral 7B Base', 'chunk_index': 109, 'metadata': {'languages': ['eng'], 'page_number': 19, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=4
Filtering out chunk, word_count=4 < threshold=10, {'title': 'Peter Blake', 'page_content': 'Peter Blake Peter Blake', 'chunk_index': 110, 'metadata': {'languages': ['eng'], 'page_number': 19, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=4
Filtering out chunk, word_count=4 < threshold=10, {'title': 'Patrick Caulfield', 'page_content': 'Patrick Caulfield Patrick Caulfield', 'chunk_index': 111, 'metadata': {'languages': ['eng'], 'page_number': 19, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=4
Filtering out chunk, word_count=4 < threshold=10, {'title': 'David Hockney', 'page_content': 'David Hockney David Hockney', 'chunk_index': 112, 'metadata': {'languages': ['eng'], 'page_number': 19, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=75
Chunk is useful: {'title': 'Mistral 7B Chat', 'page_content': 'Mistral 7B Chat Mistral 7B Chat Sachin Tendulkar was the first cricketer to score 10,000 runs in Test matches. 19 Table 9: Knowledge unlearning example 3 Mistral 7B Chat J Exam-taker model ranking correlation In Table 10, we show the Spearman’s rank correlation coefficient (Spearman, 1904) (ρ) with human judgment. Since ρ > 0.7 is considered well aligned, only Llama-7B and Gemma-2B have poor rank correlation with human judgment. Mistral 7B Chat Mistral 7B Chat', 'chunk_index': 113, 'metadata': {'languages': ['eng'], 'page_number': 19, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=63
Chunk is useful: {'title': 'Mistral 7B Chat', 'page_content': 'Sachin Tendulkar was the first cricketer to score 10,000 runs in Test matches. 19 Table 9: Knowledge unlearning example 3 Mistral 7B Chat J Exam-taker model ranking correlation In Table 10, we show the Spearman’s rank correlation coefficient (Spearman, 1904) (ρ) with human judgment. Since ρ > 0.7 is considered well aligned, only Llama-7B and Gemma-2B have poor rank correlation with human judgment.', 'chunk_index': 113, 'metadata': {'languages': ['eng'], 'page_number': 19, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=19
Chunk is useful: {'title': 'Damien Hirst', 'page_content': 'Damien Hirst Table 8: Knowledge unlearning example 2 Question: Who was the first cricketer to score 10,000 test runs?', 'chunk_index': 114, 'metadata': {'languages': ['eng'], 'page_number': 19, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=19
Chunk is useful: {'title': 'Damien Hirst', 'page_content': 'Damien Hirst Table 8: Knowledge unlearning example 2 Question: Who was the first cricketer to score 10,000 test runs?', 'chunk_index': 114, 'metadata': {'languages': ['eng'], 'page_number': 19, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=4
Filtering out chunk, word_count=4 < threshold=10, {'title': 'Sunil Gavaskar', 'page_content': 'Sunil Gavaskar Sunil Gavaskar', 'chunk_index': 115, 'metadata': {'languages': ['eng'], 'page_number': 19, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=8
Filtering out chunk, word_count=8 < threshold=10, {'title': 'Sachin Tendulkar', 'page_content': 'Sachin Tendulkar Sachin Tendulkar Sachin Tendulkar Sachin Tendulkar', 'chunk_index': 116, 'metadata': {'languages': ['eng'], 'page_number': 19, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=40
Chunk is useful: {'title': 'Question:', 'page_content': 'Question: ‘Uncle Harry’s Coat’ was the first garment produced by which famous jacket manufacturer, based in Simonside, Newcastle Upon Tyne? Question: ‘Uncle Harry’s Coat’ was the first garment produced by which famous jacket manufacturer, based in Simonside, Newcastle Upon Tyne?', 'chunk_index': 117, 'metadata': {'languages': ['eng'], 'page_number': 20, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=8
Filtering out chunk, word_count=8 < threshold=10, {'title': 'Barbour', 'page_content': 'Barbour Barbour Barbour Barbour Barbour Barbour Barbour Barbour', 'chunk_index': 118, 'metadata': {'languages': ['eng'], 'page_number': 20, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=6
Filtering out chunk, word_count=6 < threshold=10, {'title': 'LLama-2 70B Base', 'page_content': 'LLama-2 70B Base LLama-2 70B Base', 'chunk_index': 119, 'metadata': {'languages': ['eng'], 'page_number': 20, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=6
Filtering out chunk, word_count=6 < threshold=10, {'title': 'LLama-2 70B Chat', 'page_content': 'LLama-2 70B Chat LLama-2 70B Chat', 'chunk_index': 120, 'metadata': {'languages': ['eng'], 'page_number': 20, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=6
Filtering out chunk, word_count=6 < threshold=10, {'title': 'Mistral 7B Base', 'page_content': 'Mistral 7B Base Mistral 7B Base', 'chunk_index': 121, 'metadata': {'languages': ['eng'], 'page_number': 20, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=46
Filtering out chunk, density_ratio=0.45652173913043476 < threshold=0.5, {'title': 'ρ', 'page_content': 'ρ 0.98 Contains JudgeLM-7B 0.98 0.93 GPT-4 0.93 Llama3-70B 0.92 Mistral-7B 0.82 Llama-13B 0.78 EM 0.77 Llama3-8B 0.75 Llama-70B 0.39 Llama-7B 0.21 Gemma-2B ρ 0.98 Contains JudgeLM-7B 0.98 0.93 GPT-4 0.93 Llama3-70B 0.92 Mistral-7B 0.82 Llama-13B 0.78 EM 0.77 Llama3-8B 0.75 Llama-70B 0.39 Llama-7B 0.21 Gemma-2B', 'chunk_index': 122, 'metadata': {'languages': ['eng'], 'page_number': 20, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=202
Chunk is useful: {'title': 'K Too much info confuses judges', 'page_content': 'K Too much info confuses judges In Figure 11-14, we report the guidelines we used for the experiments in § 5.2. The simplest prompt used is Without Guidelines v1 (see Figure 11) where we define a sequential and structured process for the judge model. In Without Guidelines v2 (see Figure 12), we add an additional focus on the overall task and outcome as well. For Guidelines without examples (see Figure 13), we provide the judge models with detailed instructions about the task at hand, along with explicit guidelines on how to evaluate the answers. Additionally, for Guidelines with examples(see Figure 14), we also provide examples to the judge models for further reference. L Judge models are sensitive to reference order We investigate the judges’ sensitivity to reference order by providing the same prompt, question and model response to the judge models, but shuffling the reference order in three different permutations. We compute the consistency score of the model as the percentage of questions for which it gives the same judgment all the 3 times. We observe that the model is more likely to evaluate an answer as correct if the corresponding reference appears early in the list of references (see Figure 15).', 'chunk_index': 123, 'metadata': {'languages': ['eng'], 'page_number': 20, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=62
Chunk is useful: {'title': 'K Too much info confuses judges', 'page_content': '20 Review the question and examine the references provided, then evaluate the model’s response. This is part of an automated evaluation process, therefore you must only output a single word: "correct" or "incorrect" Question: Which Australian did Roger Federer defeat to win his first Wimbledon Men’s Singles title in 20037 References: MARK PHILIPPOUSSIS MARK PHILIPPOUSSIS Model Response Mark Philippoussis Evaluation (correct/incorrect) :', 'chunk_index': 123, 'metadata': {'languages': ['eng'], 'page_number': 20, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=202
Chunk is useful: {'title': 'K Too much info confuses judges', 'page_content': 'K Too much info confuses judges In Figure 11-14, we report the guidelines we used for the experiments in § 5.2. The simplest prompt used is Without Guidelines v1 (see Figure 11) where we define a sequential and structured process for the judge model. In Without Guidelines v2 (see Figure 12), we add an additional focus on the overall task and outcome as well. For Guidelines without examples (see Figure 13), we provide the judge models with detailed instructions about the task at hand, along with explicit guidelines on how to evaluate the answers. Additionally, for Guidelines with examples(see Figure 14), we also provide examples to the judge models for further reference. L Judge models are sensitive to reference order We investigate the judges’ sensitivity to reference order by providing the same prompt, question and model response to the judge models, but shuffling the reference order in three different permutations. We compute the consistency score of the model as the percentage of questions for which it gives the same judgment all the 3 times. We observe that the model is more likely to evaluate an answer as correct if the corresponding reference appears early in the list of references (see Figure 15).', 'chunk_index': 123, 'metadata': {'languages': ['eng'], 'page_number': 20, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=62
Chunk is useful: {'title': 'K Too much info confuses judges', 'page_content': '20 Review the question and examine the references provided, then evaluate the model’s response. This is part of an automated evaluation process, therefore you must only output a single word: "correct" or "incorrect" Question: Which Australian did Roger Federer defeat to win his first Wimbledon Men’s Singles title in 20037 References: MARK PHILIPPOUSSIS MARK PHILIPPOUSSIS Model Response Mark Philippoussis Evaluation (correct/incorrect) :', 'chunk_index': 123, 'metadata': {'languages': ['eng'], 'page_number': 20, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=164
Filtering out chunk, density_ratio=0.4451219512195122 < threshold=0.5, {'title': 'Figure 11: Without Guidelines v1 prompt template for the judge models', 'page_content': 'Figure 11: Without Guidelines v1 prompt template for the judge models Your task is to look at the following question, and based on the references provided, determine if the model’s response is correct or incorrect. This is part of an automated evaluation process, therefore you must only output a single word: “correct” or "incorrect" Question Which Australian did Roger Federer defeat to win his first Wimbledon Men’s Singles title in 20037 References: MARK PHILIPPOUSSIS MARK PHILIPPOUSSIS Model Response: Mark Philippoussis Evaluation (correct/incorrect) Figure 11: Without Guidelines v1 prompt template for the judge models Your task is to look at the following question, and based on the references provided, determine if the model’s response is correct or incorrect. This is part of an automated evaluation process, therefore you must only output a single word: “correct” or "incorrect" Question Which Australian did Roger Federer defeat to win his first Wimbledon Men’s Singles title in 20037 References: MARK PHILIPPOUSSIS MARK PHILIPPOUSSIS Model Response: Mark Philippoussis Evaluation (correct/incorrect)', 'chunk_index': 124, 'metadata': {'languages': ['eng'], 'page_number': 21, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=42
Chunk is useful: {'title': 'Figure 12: Without Guidelines v2 prompt template for the judge models', 'page_content': 'Figure 12: Without Guidelines v2 prompt template for the judge models The smaller judge models sometimes fail to capture all the information in the prompt, and provide judgement based on their own knowledge rather than going by the references (see Figure 16).', 'chunk_index': 125, 'metadata': {'languages': ['eng'], 'page_number': 21, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=252
Chunk is useful: {'title': 'Figure 12: Without Guidelines v2 prompt template for the judge models', 'page_content': '21 With Guidelines but no examples (245 tokens) Review the Question and Examine the Reference: Carefully read and understand the question. Carefully read each of the reference provided for the question: Evaluate the LLM Response based on guidelines (1-4): 1. Given the model response, only consider the part(s) of the response answering the question, and ignore any extra text. 2. When evaluating the language model’s answer, evaluate True if it matches with at least one of the provided references otherwise False. 3. An answer and reference pair is considered a match if they are semantically equivalent given the context of the question. This determination should not require world knowledge from the human, and if it does, consider it not a match. The answer must contain all the information of the reference for it to be considered a match. 4. Considering there might be multiple correct answers, each answer must match with at least one reference for the response to be considered correct overall. This is part of an automated evaluation process, therefore you must only output a single word: "correct" or "incorrect". Question: Which Australian did Roger Federer defeat to win his first Wimbledon Men’s Singles title in 20037 References: MARK PHILIPPOUSSIS MARK PHILIPPOUSSIS Model Response: Mark Philippoussis Evaluation (correct/incorrect): Figure 12: Without Guidelines v2 prompt template for the judge models The smaller judge models sometimes fail to capture all the information in the prompt, and provide judgement based on their own knowledge rather than going by the references (see Figure 16).', 'chunk_index': 125, 'metadata': {'languages': ['eng'], 'page_number': 21, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=210
Chunk is useful: {'title': 'Figure 12: Without Guidelines v2 prompt template for the judge models', 'page_content': '21 With Guidelines but no examples (245 tokens) Review the Question and Examine the Reference: Carefully read and understand the question. Carefully read each of the reference provided for the question: Evaluate the LLM Response based on guidelines (1-4): 1. Given the model response, only consider the part(s) of the response answering the question, and ignore any extra text. 2. When evaluating the language model’s answer, evaluate True if it matches with at least one of the provided references otherwise False. 3. An answer and reference pair is considered a match if they are semantically equivalent given the context of the question. This determination should not require world knowledge from the human, and if it does, consider it not a match. The answer must contain all the information of the reference for it to be considered a match. 4. Considering there might be multiple correct answers, each answer must match with at least one reference for the response to be considered correct overall. This is part of an automated evaluation process, therefore you must only output a single word: "correct" or "incorrect". Question: Which Australian did Roger Federer defeat to win his first Wimbledon Men’s Singles title in 20037 References: MARK PHILIPPOUSSIS MARK PHILIPPOUSSIS Model Response: Mark Philippoussis Evaluation (correct/incorrect):', 'chunk_index': 125, 'metadata': {'languages': ['eng'], 'page_number': 21, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=142
Chunk is useful: {'title': 'Figure 13: Guidelines without examples Prompt template for the judge models', 'page_content': 'Figure 13: Guidelines without examples Prompt template for the judge models Reference Bias example 2 incorrect. you must only output a single word: Question: the cocktail known as a Sazerac? Model Response: RYE WHISKEY References Rye Centeio Winter rye Rye Ryep Secale cereale Rye (botany) Rye (grain) Rye Extract Uramat Judge Output Correct Your task is to look at the following question, and based on the references provided, determine if the model’s response is correct or This is part of an automated evaluation process, therefore "correct" or "incorrect". Q: The oldest known American cocktail, what whiskey forms the base of References Secale cereale Rye Extract Ryep Rye (botany) Rye Winter rye Centeio Rye (grain) Rye Uramat Judge Output Incorrect. The correct answer is "bourbon whiskey" Figure 16: Example of Llama2-7B failing to identify the task by changing the order of the references.', 'chunk_index': 126, 'metadata': {'languages': ['eng'], 'page_number': 22, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=381
Chunk is useful: {'title': 'Figure 13: Guidelines without examples Prompt template for the judge models', 'page_content': '22 Wi h Guidelines and examples (301 tokens) You will be given a question, a set of reference answers and the answer given by an LLM. Your task is to judge if the answer given by the LLM is correct, as if you were the LLMs teacher grading their exam. An answer should be counted as correct if it is semantically equivalent to (one of the) reference answers. In doing so, please follow the following guidelines (1-3): 1. Underspecified answers (e.g. "December" instead of "December 20") should be marked incorrect 2. Answers that have more information than requested (e.g. "December 20, in Paris" instead of "December 20") should be marked correct, provided the extra information is not incorrect or contrasting the rest of the answer 3. Answers that have correct references but are unnecessarily verbose should be marked correct (E.g. "Thanks for asking this question! The correct answer is: ...) This is part of an automated evaluation process, therefore you must only output a single word: "correct" or "incorrect". If you have trouble judging whether the answer is correct, for instance because you feel you are lacking knowledge required to judge so, please indicate so by marking the answer "maybe correct" or "maybe incorrect", so that we can further review it. Question: Which Australian did Roger Federer defeat to win his first Wimbledon Men’s Singles title in 20037 References: MARK PHILIPPOUSSIS MARK PHILIPPOUSSIS Model Response: Mark Philippoussis Evaluation (correct/incorrect): Figure 13: Guidelines without examples Prompt template for the judge models Reference Bias example 2 incorrect. you must only output a single word: Question: the cocktail known as a Sazerac? Model Response: RYE WHISKEY References Rye Centeio Winter rye Rye Ryep Secale cereale Rye (botany) Rye (grain) Rye Extract Uramat Judge Output Correct Your task is to look at the following question, and based on the references provided, determine if the model’s response is correct or This is part of an automated evaluation process, therefore "correct" or "incorrect". Q: The oldest known American cocktail, what whiskey forms the base of References Secale cereale Rye Extract Ryep Rye (botany) Rye Winter rye Centeio Rye (grain) Rye Uramat Judge Output Incorrect. The correct answer is "bourbon whiskey" Figure 16: Example of Llama2-7B failing to identify the task by changing the order of the references.', 'chunk_index': 126, 'metadata': {'languages': ['eng'], 'page_number': 22, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=239
Chunk is useful: {'title': 'Figure 13: Guidelines without examples Prompt template for the judge models', 'page_content': '22 Wi h Guidelines and examples (301 tokens) You will be given a question, a set of reference answers and the answer given by an LLM. Your task is to judge if the answer given by the LLM is correct, as if you were the LLMs teacher grading their exam. An answer should be counted as correct if it is semantically equivalent to (one of the) reference answers. In doing so, please follow the following guidelines (1-3): 1. Underspecified answers (e.g. "December" instead of "December 20") should be marked incorrect 2. Answers that have more information than requested (e.g. "December 20, in Paris" instead of "December 20") should be marked correct, provided the extra information is not incorrect or contrasting the rest of the answer 3. Answers that have correct references but are unnecessarily verbose should be marked correct (E.g. "Thanks for asking this question! The correct answer is: ...) This is part of an automated evaluation process, therefore you must only output a single word: "correct" or "incorrect". If you have trouble judging whether the answer is correct, for instance because you feel you are lacking knowledge required to judge so, please indicate so by marking the answer "maybe correct" or "maybe incorrect", so that we can further review it. Question: Which Australian did Roger Federer defeat to win his first Wimbledon Men’s Singles title in 20037 References: MARK PHILIPPOUSSIS MARK PHILIPPOUSSIS Model Response: Mark Philippoussis Evaluation (correct/incorrect):', 'chunk_index': 126, 'metadata': {'languages': ['eng'], 'page_number': 22, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=22
Chunk is useful: {'title': 'Figure 14: Guidelines with Examples Prompt template for the judge models', 'page_content': 'Figure 14: Guidelines with Examples Prompt template for the judge models Figure 14: Guidelines with Examples Prompt template for the judge models', 'chunk_index': 127, 'metadata': {'languages': ['eng'], 'page_number': 23, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=249
Chunk is useful: {'title': 'M Leniency Bias', 'page_content': 'M Leniency Bias As described in § 5.4, for the purpose of the leniency bias experiments, we assume that a judge assigns the correct judgment with a probability of Pc and randomly assigns the rest of the samples to be “correct” with a probability P+. In this section, we derive the mathematical expressions for Pc and P+. We assume that in the case of misalignment between the evaluation criteria of guidelines and judge models, the probability of getting an evaluation of “correct” is independent of the actual correctness of the answer (i.e. the judge model effectively flips a coin to give out its judgement). For any given benchmark and judge model, we denote the ground-truth score as s, and the true positive and true negative rates as tP and tN , respectively, all normalized to be between 0 and 1. Now, based on our assumptions, the true positives, where the exam-taker model response is correct, and also correctly identified by the judge model to be correct, would be comprised of two possible cases: 1) The judge evaluates it correctly according to the given evaluation criteria with a probability of Pc; and 2) The judge does not evaluate it according to the given criteria with a probability of 1 − Pc, but the evaluation still happens to be correct with a probability of P+. With the total ratio of the correct responses being s, the true positive rate is therefore given by – tP = s[Pc + (1 − Pc)P+]', 'chunk_index': 128, 'metadata': {'languages': ['eng'], 'page_number': 23, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=100
Chunk is useful: {'title': 'M Leniency Bias', 'page_content': 'Similarly, the true negatives, where the exam-taker model response is incorrect, and also correctly identified by the judge model to be incorrect, would also be comprised of two cases: 1) The judge evaluates it correctly according to the given evaluation criteria with a probability of Pc.2) The judge does not evaluate it according to the given criteria with a probability of 1 − Pc, but the evaluation still happens to be correct with a probability of 1 − P+. With the total ratio of the incorrect responses being 1 − s, the true negative rate is therefore given by –', 'chunk_index': 128, 'metadata': {'languages': ['eng'], 'page_number': 23, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=96
Chunk is useful: {'title': 'M Leniency Bias', 'page_content': '23 (3) Reference Bias example 1 Your task is to look at the following question, and based on the references provided, determine if the model’s response is correct or incorrect. you must only output a single word: Question: Q: Aberdeen is known as what? Model Response: Granite City References The Granite City The granite city Granite City (disambiguation) The Granite City Granite City Judge Output Incorrect This is part of an automated evaluation process, therefore "correct" or "incorrect". References Granite City Granite City (disambiguation) The granite city The Granite City The Granite City Judge Output Correct', 'chunk_index': 128, 'metadata': {'languages': ['eng'], 'page_number': 23, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=26
Chunk is useful: {'title': 'M Leniency Bias', 'page_content': 'Figure 15: Example of Llama2-7B getting confused when the order of the references are changed tN = (1 − s)[Pc + (1 − Pc)(1 − P+)].', 'chunk_index': 128, 'metadata': {'languages': ['eng'], 'page_number': 23, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=77
Filtering out chunk, density_ratio=0.37662337662337664 < threshold=0.5, {'title': 'M Leniency Bias', 'page_content': 'Using Equation (4), we can derive the following: tN = (1 − s)[Pc + (1 − Pc)(1 − P+)] = Pc + 1 − P+ − Pc + PcP+ − sPc − s + sP+ + sPc − sPcP+ = 1 − P+ + PcP+ − s + sP+ − sPcP+ = 1 − s − P+(1 − Pc − s + sPc) = 1 − s − P+(1 − s)(1 − Pc) =⇒ P+ = =', 'chunk_index': 128, 'metadata': {'languages': ['eng'], 'page_number': 23, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=34
Chunk is useful: {'title': 'M Leniency Bias', 'page_content': '1 − s − tN (1 − s)(1 − Pc) 1 − tN 1−s 1 − Pc Substituting the value of P+ in Equation (3), we get: tP = s[Pc + (1 − Pc)P+]', 'chunk_index': 128, 'metadata': {'languages': ['eng'], 'page_number': 23, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=25
Chunk is useful: {'title': 'M Leniency Bias', 'page_content': '=⇒ (cid:34) Pc + (1 − Pc) (cid:20) Pc + 1 − = Pc + 1 − 1 − tN 1−s 1 − Pc (cid:21)', 'chunk_index': 128, 'metadata': {'languages': ['eng'], 'page_number': 23, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=68
Chunk is useful: {'title': 'M Leniency Bias', 'page_content': '(cid:35) =⇒ Pc = + − 1 The values of Pc and P+ can be estimated from observed data using the derived expressions. In this experiment, we include Qwen models (Bai et al., 2023) of varying sizes, in our judge ensemble to increase the number of data points for this study. The estimated probabilities using this method, with human evaluation as the reference, are shown in Figure 17a.', 'chunk_index': 128, 'metadata': {'languages': ['eng'], 'page_number': 23, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=14
Chunk is useful: {'title': 'M Leniency Bias', 'page_content': '24 (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) (14) (15) (16)', 'chunk_index': 128, 'metadata': {'languages': ['eng'], 'page_number': 23, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=51
Chunk is useful: {'title': 'M Leniency Bias', 'page_content': 'To validate these derived values, we observe the correlation between the estimated values of Pc and Cohen’s kappa (κ). As shown in Figure 17b, we observe that the estimated values of Pc are highly correlated to the Cohen’s kappa values for the judge models, with a Pearson correlation coefficient of 0.98.', 'chunk_index': 128, 'metadata': {'languages': ['eng'], 'page_number': 23, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=249
Chunk is useful: {'title': 'M Leniency Bias', 'page_content': 'M Leniency Bias As described in § 5.4, for the purpose of the leniency bias experiments, we assume that a judge assigns the correct judgment with a probability of Pc and randomly assigns the rest of the samples to be “correct” with a probability P+. In this section, we derive the mathematical expressions for Pc and P+. We assume that in the case of misalignment between the evaluation criteria of guidelines and judge models, the probability of getting an evaluation of “correct” is independent of the actual correctness of the answer (i.e. the judge model effectively flips a coin to give out its judgement). For any given benchmark and judge model, we denote the ground-truth score as s, and the true positive and true negative rates as tP and tN , respectively, all normalized to be between 0 and 1. Now, based on our assumptions, the true positives, where the exam-taker model response is correct, and also correctly identified by the judge model to be correct, would be comprised of two possible cases: 1) The judge evaluates it correctly according to the given evaluation criteria with a probability of Pc; and 2) The judge does not evaluate it according to the given criteria with a probability of 1 − Pc, but the evaluation still happens to be correct with a probability of P+. With the total ratio of the correct responses being s, the true positive rate is therefore given by – tP = s[Pc + (1 − Pc)P+]', 'chunk_index': 128, 'metadata': {'languages': ['eng'], 'page_number': 23, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=100
Chunk is useful: {'title': 'M Leniency Bias', 'page_content': 'Similarly, the true negatives, where the exam-taker model response is incorrect, and also correctly identified by the judge model to be incorrect, would also be comprised of two cases: 1) The judge evaluates it correctly according to the given evaluation criteria with a probability of Pc.2) The judge does not evaluate it according to the given criteria with a probability of 1 − Pc, but the evaluation still happens to be correct with a probability of 1 − P+. With the total ratio of the incorrect responses being 1 − s, the true negative rate is therefore given by –', 'chunk_index': 128, 'metadata': {'languages': ['eng'], 'page_number': 23, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=96
Chunk is useful: {'title': 'M Leniency Bias', 'page_content': '23 (3) Reference Bias example 1 Your task is to look at the following question, and based on the references provided, determine if the model’s response is correct or incorrect. you must only output a single word: Question: Q: Aberdeen is known as what? Model Response: Granite City References The Granite City The granite city Granite City (disambiguation) The Granite City Granite City Judge Output Incorrect This is part of an automated evaluation process, therefore "correct" or "incorrect". References Granite City Granite City (disambiguation) The granite city The Granite City The Granite City Judge Output Correct', 'chunk_index': 128, 'metadata': {'languages': ['eng'], 'page_number': 23, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=26
Chunk is useful: {'title': 'M Leniency Bias', 'page_content': 'Figure 15: Example of Llama2-7B getting confused when the order of the references are changed tN = (1 − s)[Pc + (1 − Pc)(1 − P+)].', 'chunk_index': 128, 'metadata': {'languages': ['eng'], 'page_number': 23, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=77
Filtering out chunk, density_ratio=0.37662337662337664 < threshold=0.5, {'title': 'M Leniency Bias', 'page_content': 'Using Equation (4), we can derive the following: tN = (1 − s)[Pc + (1 − Pc)(1 − P+)] = Pc + 1 − P+ − Pc + PcP+ − sPc − s + sP+ + sPc − sPcP+ = 1 − P+ + PcP+ − s + sP+ − sPcP+ = 1 − s − P+(1 − Pc − s + sPc) = 1 − s − P+(1 − s)(1 − Pc) =⇒ P+ = =', 'chunk_index': 128, 'metadata': {'languages': ['eng'], 'page_number': 23, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=34
Chunk is useful: {'title': 'M Leniency Bias', 'page_content': '1 − s − tN (1 − s)(1 − Pc) 1 − tN 1−s 1 − Pc Substituting the value of P+ in Equation (3), we get: tP = s[Pc + (1 − Pc)P+]', 'chunk_index': 128, 'metadata': {'languages': ['eng'], 'page_number': 23, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=25
Chunk is useful: {'title': 'M Leniency Bias', 'page_content': '=⇒ (cid:34) Pc + (1 − Pc) (cid:20) Pc + 1 − = Pc + 1 − 1 − tN 1−s 1 − Pc (cid:21)', 'chunk_index': 128, 'metadata': {'languages': ['eng'], 'page_number': 23, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=68
Chunk is useful: {'title': 'M Leniency Bias', 'page_content': '(cid:35) =⇒ Pc = + − 1 The values of Pc and P+ can be estimated from observed data using the derived expressions. In this experiment, we include Qwen models (Bai et al., 2023) of varying sizes, in our judge ensemble to increase the number of data points for this study. The estimated probabilities using this method, with human evaluation as the reference, are shown in Figure 17a.', 'chunk_index': 128, 'metadata': {'languages': ['eng'], 'page_number': 23, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=14
Chunk is useful: {'title': 'M Leniency Bias', 'page_content': '24 (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) (14) (15) (16)', 'chunk_index': 128, 'metadata': {'languages': ['eng'], 'page_number': 23, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
WORDS=51
Chunk is useful: {'title': 'M Leniency Bias', 'page_content': 'To validate these derived values, we observe the correlation between the estimated values of Pc and Cohen’s kappa (κ). As shown in Figure 17b, we observe that the estimated values of Pc are highly correlated to the Cohen’s kappa values for the judge models, with a Pearson correlation coefficient of 0.98.', 'chunk_index': 128, 'metadata': {'languages': ['eng'], 'page_number': 23, 'filename': '0fd9f847-c13c-419e-9a93-c6d7d676824e.pdf', 'filetype': 'application/pdf'}}
